
@article{kim_economics_2020,
	title = {Economics of Fog Computing: Interplay Among Infrastructure and Service Providers, Users, and Edge Resource Owners},
	volume = {19},
	issn = {1558-0660},
	doi = {10.1109/TMC.2019.2925797},
	shorttitle = {Economics of Fog Computing},
	abstract = {Fog computing is a paradigm which brings computing, storage, and networking closer to end users and end devices for better service provisioning. One of the crucial factors in the success of fog computing is on how to incentivize the individual users' edge resources and provide them to end users such that fog computing is economically beneficial to all involved economic players. In this paper, we model and analyze a market of fog computing, from which we aim at drawing practical implications to uncover how the fog computing market should operate. To this end, we conduct an economic analysis of such user-oriented fog computing by modeling a market consisting of Infrastructure and Service Provider ({ISP}), end Service Users ({SUs}), and Edge Resource Owners ({EROs}) as a non-cooperative game. In this market, {ISP}, which provides a platform for fog computing, behaves as a mediator or a broker which leases {EROs}' edge resources and provides various services to {SUs}. In our model, a two-stage dynamic game is used where in each stage, there exists a dynamic game, one for between {ISP} and {EROs} and another for between {ISP} and {SUs}, to model the market more practically. Despite this complex game structure, we provide a closed-form equilibrium analysis which gives an insight on how much economic benefit is obtained by {ISP}, {SUs}, and {EROs} from user-oriented fog computing under what conditions, and we figure out the economic factors that have a significant impact on the success of fog computing.},
	pages = {2609--2622},
	number = {11},
	journaltitle = {{IEEE} Transactions on Mobile Computing},
	author = {Kim, Daewoo and Lee, Hyojung and Song, Hyungseok and Choi, Nakjung and Yi, Yung},
	date = {2020-11},
	note = {Conference Name: {IEEE} Transactions on Mobile Computing},
	keywords = {Edge computing, Business, Computational modeling, Economics, Edge network, fog computing, game theory, Games, {III}-V semiconductor materials, Indium phosphide, network economics},
	file = {IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/729CK3JW/8766848.html:text/html;Economics of Fog Computing\: Interplay Among Infrastructure and Service Providers, Users, and Edge Resource Owners:/home/volodia/Zotero/storage/XXZVIMHM/kim2019.pdf.pdf:application/pdf},
}

@inproceedings{samanta_incentivizing_2019,
	title = {Incentivizing Microservices for Online Resource Sharing in Edge Clouds},
	doi = {10.1109/ICDCS.2019.00049},
	abstract = {The microservice architecture provides high agility, making it a suitable choice for implementing edge cloud services. Provisioning microservices at the network edge requires the dynamic allocation of resources. However, due to the resource limitation in the edge cloud environment, there is no guarantee that enough resources are always available upon a microservice's requests. In this paper, we design an online auction-based mechanism to incentivize microservices to spare their occupied resources so that the edge cloud platform can reclaim them and reallocate them to other microservices that need resources. We firstly design a single-stage auction that determines the winning bids to satisfy the resource demands in polynomial time, while calculating the payments. Then, we design an online framework to tie a series of such single-stage auctions into a multi-stage online mechanism without requiring the knowledge of future bids and demands. Via rigorous analysis, we exhibit that our mechanism design achieves truthful bidding and individual rationality, with a constant competitive ratio regarding the social cost of the system in the long run. Finally, we verify the practical performance of our mechanism through extensive simulations.},
	eventtitle = {2019 {IEEE} 39th International Conference on Distributed Computing Systems ({ICDCS})},
	pages = {420--430},
	booktitle = {2019 {IEEE} 39th International Conference on Distributed Computing Systems ({ICDCS})},
	author = {Samanta, Amit and Jiao, Lei and Mühlhäuser, Max and Wang, Lin},
	date = {2019-07},
	note = {{ISSN}: 2575-8411},
	keywords = {Cloud computing, Edge computing, Resource management, Economics, Estimation, Dynamic scheduling, Microservice, Online algorithm, Pricing, Resource sharing},
	file = {IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/IKG559L9/8885350.html:text/html;Incentivizing Microservices for Online Resource Sharing in Edge Clouds:/home/volodia/Zotero/storage/6FYRKJ7T/samanta2019.pdf.pdf:application/pdf},
}

@article{tasiopoulos_fogspot_2019,
	title = {{FogSpot}: Spot Pricing for Application Provisioning in Edge/Fog Computing},
	issn = {1939-1374},
	doi = {10.1109/TSC.2019.2895037},
	shorttitle = {{FogSpot}},
	abstract = {An increasing number of Low Latency Applications ({LLAs}) in the entertainment, {IoT}, and automotive domains require response times that challenge the traditional application provisioning using distant Data Centres. Fog computing paradigm extends cloud computing at the edge and middle-tier locations of the network, providing response times an order of magnitude smaller than those that can be achieved by the current "client-to-cloud" network model. Here, we address the challenges of provisioning heavily stateful {LLA} in the setting where fog infrastructure consists of third-party computing resources, i.e., cloudlets, that comes in the form of "data centres in the box". We introduce {FogSpot}, a charging mechanism for on-path, on-demand, application provisioning. In {FogSpot}, cloudlets offer their resources in the form of Virtual Machines ({VMs}) via markets, collocated with the cloudlets, that interact with forwarded users' application requests for {VMs} in real time. {FogSpot} associates each cloudlet with a spot price based on current application requests. The proposed mechanism's design takes into account the characteristics of cloudlets' resources, such as their limited elasticity, and {LLAs}' attributes, like the expected {QoS} gain and engagement duration. Lastly, {FogSpot} guarantees end users' requests truthfulness while focusing in maximising either each cloudlet's revenue or resource utilisation.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Services Computing},
	author = {Tasiopoulos, Argyrios and Ascigil, Onur and Psaras, Ioannis and Toumpis, Stavros and Pavlou, George},
	date = {2019},
	note = {Conference Name: {IEEE} Transactions on Services Computing},
	keywords = {Cloud computing, Edge computing, Data centers, Task analysis, Quality of service, Pricing, Elasticity},
	file = {IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/MMNBG3IY/8625439.html:text/html;Submitted Version:/home/volodia/Zotero/storage/HS2875WE/Tasiopoulos et al. - 2019 - FogSpot Spot Pricing for Application Provisioning.pdf:application/pdf;FogSpot\: Spot Pricing for Application Provisioning in Edge/Fog Computing:/home/volodia/Zotero/storage/6DEI24LH/10.1109@TSC.2019.2895037.pdf.pdf:application/pdf},
}

@inproceedings{fawcett_combinatorial_2016,
	title = {Combinatorial Auction-Based Resource Allocation in the Fog},
	doi = {10.1109/EWSDN.2016.16},
	abstract = {Network service composition is becoming increasingly flexible, thanks in part to advances in virtualisation and cloud technologies. As these penetrate further into networks, providers are often looking to leverage this infrastructure to improve their service delivery. This desire poses a number of obstacles, including a diversity in device capabilities and the need for a value exchange mechanism. In this demonstration, we present a platform that seeks to address a selection of these challenges.},
	eventtitle = {2016 Fifth European Workshop on Software-Defined Networks ({EWSDN})},
	pages = {62--67},
	booktitle = {2016 Fifth European Workshop on Software-Defined Networks ({EWSDN})},
	author = {Fawcett, Lyndon and Broadbent, Matthew and Race, Nicholas},
	date = {2016-10},
	note = {{ISSN}: 2379-0369},
	keywords = {Cloud computing, Edge computing, Fog Computing, Resource management, Containers, Combinatorial Auctions, Conferences, Europe, Network Virtualisation, Orchestration, Provisioning},
	file = {IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/R7EHW86M/7956056.html:text/html;Accepted Version:/home/volodia/Zotero/storage/2J77S9SI/Fawcett et al. - 2016 - Combinatorial Auction-Based Resource Allocation in.pdf:application/pdf;Combinatorial Auction-Based Resource Allocation in the Fog:/home/volodia/Zotero/storage/YYHMJHT6/fawcett2016.pdf.pdf:application/pdf},
}

@article{bermbach_auctionwhisk_2021,
	title = {{AuctionWhisk}: Using an Auction-Inspired Approach for Function Placement in Serverless Fog Platforms},
	url = {http://arxiv.org/abs/2108.13222},
	shorttitle = {{AuctionWhisk}},
	abstract = {The Function-as-a-Service ({FaaS}) paradigm has a lot of potential as a computing model for fog environments comprising both cloud and edge nodes. When the request rate exceeds capacity limits at the edge, some functions need to be offloaded from the edge towards the cloud. In this paper, we present an auction-inspired approach in which application developers bid on resources while fog nodes decide locally which functions to execute and which to offload in order to maximize revenue. We evaluate our approach through a number of simulations, our proof-of-concept prototype {AuctionWhisk}, and a number of experiments with {AuctionWhisk}.},
	journaltitle = {{arXiv}:2108.13222 [cs]},
	author = {Bermbach, David and Bader, Jonathan and Hasenburg, Jonathan and Pfandzelter, Tobias and Thamsen, Lauritz},
	urldate = {2021-11-15},
	date = {2021-08-30},
	eprinttype = {arxiv},
	eprint = {2108.13222},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv Fulltext PDF:/home/volodia/Zotero/storage/SBG2QP3Q/Bermbach et al. - 2021 - AuctionWhisk Using an Auction-Inspired Approach f.pdf:application/pdf;arXiv.org Snapshot:/home/volodia/Zotero/storage/6Y99HU78/2108.html:text/html},
}

@article{mampage_holistic_2021,
	title = {A Holistic View on Resource Management in Serverless Computing Environments: Taxonomy and Future Directions},
	url = {http://arxiv.org/abs/2105.11592},
	shorttitle = {A Holistic View on Resource Management in Serverless Computing Environments},
	abstract = {Serverless computing has emerged as an attractive deployment option for cloud applications in recent times. The unique features of this computing model include, rapid auto-scaling, strong isolation, fine-grained billing options and access to a massive service ecosystem which autonomously handles resource management decisions. This model is increasingly being explored for deployments in geographically distributed edge and fog computing networks as well, due to these characteristics. Effective management of computing resources has always gained a lot of attention among researchers. The need to automate the entire process of resource provisioning, allocation, scheduling, monitoring and scaling, has resulted in the need for specialized focus on resource management under the serverless model. In this article, we identify the major aspects covering the broader concept of resource management in serverless environments and propose a taxonomy of elements which influence these aspects, encompassing characteristics of system design, workload attributes and stakeholder expectations. We take a holistic view on serverless environments deployed across edge, fog and cloud computing networks. We also analyse existing works discussing aspects of serverless resource management using this taxonomy. This article further identifies gaps in literature and highlights future research directions for improving capabilities of this computing model.},
	journaltitle = {{arXiv}:2105.11592 [cs]},
	author = {Mampage, Anupama and Karunasekera, Shanika and Buyya, Rajkumar},
	urldate = {2021-12-01},
	date = {2021-05-31},
	eprinttype = {arxiv},
	eprint = {2105.11592},
	keywords = {A.1, C.m, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv.org Snapshot:/home/volodia/Zotero/storage/W3P28Q4P/2105.html:text/html;arXiv Fulltext PDF:/home/volodia/Zotero/storage/2FJJPQZN/Mampage et al. - 2021 - A Holistic View on Resource Management in Serverle.pdf:application/pdf},
}

@inproceedings{aslanpour_serverless_2021,
	location = {New York, {NY}, {USA}},
	title = {Serverless Edge Computing: Vision and Challenges},
	isbn = {978-1-4503-8956-3},
	url = {https://doi.org/10.1145/3437378.3444367},
	doi = {10.1145/3437378.3444367},
	series = {{ACSW} '21},
	shorttitle = {Serverless Edge Computing},
	abstract = {Born from a need for a pure “pay-per-use” model and highly scalable platform, the “Serverless” paradigm emerged and has the potential to become a dominant way of building cloud applications. Although it was originally designed for cloud environments, Serverless is finding its position in the Edge Computing landscape, aiming to bring computational resources closer to the data source. That is, Serverless is crossing cloud borders to assess its merits in Edge computing, whose principal partner will be the Internet of Things ({IoT}) applications. This move sounds promising as Serverless brings particular benefits such as eliminating always-on services causing high electricity usage, for instance. However, the community is still hesitant to uptake Serverless Edge Computing because of the cloud-driven design of current Serverless platforms, and distinctive characteristics of edge landscape and {IoT} applications. In this paper, we evaluate both sides to shed light on the Serverless new territory. Our in-depth analysis promotes a broad vision for bringing Serverless to the Edge Computing. It also issues major challenges for Serverless to be met before entering Edge computing.},
	pages = {1--10},
	booktitle = {2021 Australasian Computer Science Week Multiconference},
	publisher = {Association for Computing Machinery},
	author = {Aslanpour, Mohammad S. and Toosi, Adel N. and Cicconetti, Claudio and Javadi, Bahman and Sbarski, Peter and Taibi, Davide and Assuncao, Marcos and Gill, Sukhpal Singh and Gaire, Raj and Dustdar, Schahram},
	urldate = {2021-11-15},
	date = {2021-02-01},
	file = {Serverless Edge Computing\: Vision and Challenges:/home/volodia/Zotero/storage/IPK7XN74/aslanpour2021.pdf.pdf:application/pdf},
}

@article{salaht_overview_2020,
	title = {An Overview of Service Placement Problem in Fog and Edge Computing},
	volume = {53},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3391196},
	doi = {10.1145/3391196},
	abstract = {To support the large and various applications generated by the Internet of Things ({IoT}), Fog Computing was introduced to complement the Cloud Computing and offer Cloud-like services at the edge of the network with low latency and real-time responses. Large-scale, geographical distribution, and heterogeneity of edge computational nodes make service placement in such infrastructure a challenging issue. Diversity of user expectations and {IoT} devices characteristics also complicate the deployment problem. This article presents a survey of current research conducted on Service Placement Problem ({SPP}) in the Fog/Edge Computing. Based on a new classification scheme, a categorization of current proposals is given and identified issues and challenges are discussed.},
	pages = {65:1--65:35},
	number = {3},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Salaht, Farah Aït and Desprez, Frédéric and Lebre, Adrien},
	urldate = {2021-11-15},
	date = {2020-06-12},
	keywords = {edge computing, classification, deployment taxonomy, Fog computing, optimization, service placement},
	file = {Submitted Version:/home/volodia/Zotero/storage/PW85BMSN/Salaht et al. - 2020 - An Overview of Service Placement Problem in Fog an.pdf:application/pdf;An Overview of Service Placement Problem in Fog and Edge Computing:/home/volodia/Zotero/storage/FH4Y59JP/salaht2020.pdf.pdf:application/pdf},
}

@inproceedings{palade_evaluation_2019,
	title = {An Evaluation of Open Source Serverless Computing Frameworks Support at the Edge},
	volume = {2642-939X},
	doi = {10.1109/SERVICES.2019.00057},
	abstract = {The proliferation of Internet of Things ({IoT}) and the success of resource-rich cloud services have pushed the data processing horizon towards the edge of the network. This has the potential to address bandwidth costs, and latency, availability and data privacy concerns. Serverless computing, a cloud computing model for stateless and event-driven applications, promises to further improve Quality of Service ({QoS}) by eliminating the burden of always-on infrastructure through ephemeral containers. Open source serverless frameworks have been introduced to avoid the vendor lock-in and computation restrictions of public cloud platforms and to bring the power of serverless computing to on-premises deployments. In an {IoT} environment, these frameworks can leverage the computational capabilities of devices in the local network to further improve {QoS} of applications delivered to the user. However, these frameworks have not been evaluated in a resource-constrained, edge computing environment. In this work we evaluate four open source serverless frameworks, namely, Kubeless, Apache {OpenWhisk}, {OpenFaaS}, Knative. Each framework is installed on a bare-metal, single master, Kubernetes cluster. We use the {JMeter} framework to evaluate the response time, throughput and success rate of functions deployed using these frameworks under different workloads. The evaluation results are presented and open research opportunities are discussed.},
	eventtitle = {2019 {IEEE} World Congress on Services ({SERVICES})},
	pages = {206--211},
	booktitle = {2019 {IEEE} World Congress on Services ({SERVICES})},
	author = {Palade, Andrei and Kazmi, Aqeel and Clarke, Siobhán},
	date = {2019-07},
	note = {{ISSN}: 2642-939X},
	keywords = {Edge computing, {FaaS}, Computational modeling, serverless computing, Task analysis, Runtime, Quality of service, containers, Containers, Apache Openwhisk, Docker, Function as a Service, function composition, {JMeter}, Knative, kubeless, Kubernetes, Measurement, {OpenFaaS}, opensource serverless computing frameworks, {QoS}, quantitative evaluation},
	file = {IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/38C4EDNH/8817155.html:text/html;Submitted Version:/home/volodia/Zotero/storage/LQTWIETZ/Palade et al. - 2019 - An Evaluation of Open Source Serverless Computing .pdf:application/pdf;An Evaluation of Open Source Serverless Computing Frameworks Support at the Edge:/home/volodia/Zotero/storage/5PPLMJSZ/palade2019.pdf.pdf:application/pdf},
}

@thesis{bolton_intersection_2021,
	title = {The Intersection of Function-as-a-Service and Stream Computing},
	url = {https://oaktrust.library.tamu.edu/handle/1969.1/194395},
	abstract = {With recent advancements in the field of computing including the emergence of cloud computing, the consumption and accessibility of computational resources have increased drastically. Although there have been significant movements towards more sustainable computing, there are many more steps to be taken to decrease the amount of energy consumed and greenhouse gases released from the computing sector. Historically, the switch from on-premises computing to cloud computing has led to less energy consumption through the design of efficient data centers. By releasing direct control of the hardware that their software is run on, an organization can also increase efficiency and reduce costs. A new development in cloud computing has been serverless computing. Even though the term "serverless" is a misnomer because all applications are still executed on servers, serverless lets an organization resign another level of control, managing instances of virtual machines, to their cloud provider in order to reduce their cost. The cloud provider then provisions resources on-demand enabling less idle time. This reduction of idle time is a direct reduction of computing resources used, therefore resulting in a decrease in energy consumption. One form of serverless computing, Function-as-a-Service ({FaaS}), may have a promising future replacing some stream computing applications in order to increase efficiency and reduce waste. To explore these possibilities, the development of a stream processing application using traditional methods through Kafka Streams and {FaaS} through {AWS} Lambda was completed in order to demonstrate that {FaaS} can be used for stateless stream processing.},
	type = {Thesis},
	author = {Bolton, Trevor A.},
	urldate = {2021-12-01},
	date = {2021-07-24},
	langid = {english},
	note = {Accepted: 2021-07-24T00:30:13Z},
	file = {Snapshot:/home/volodia/Zotero/storage/ZFT82WXE/194395.html:text/html;Full Text PDF:/home/volodia/Zotero/storage/M729D9UB/Bolton - 2021 - The Intersection of Function-as-a-Service and Stre.pdf:application/pdf},
}

@online{noauthor_journey_nodate,
	title = {Journey to Event Driven – Part 3: The Affinity Between Events, Streams and Serverless},
	url = {https://www.confluent.io/blog/journey-to-event-driven-part-3-affinity-between-events-streams-serverless},
	shorttitle = {Journey to Event Driven – Part 3},
	abstract = {The synergy of {FaaS} and the event streaming platform is a natural fit when we think about domain modeling and organizational needs as they change over time. Whether it’s business processes, data models, technology, cloud or just the organization itself, evolution is inevitable and needs to be embraced.},
	titleaddon = {Confluent},
	urldate = {2021-12-01},
	langid = {english},
	file = {Snapshot:/home/volodia/Zotero/storage/L67ZKBFH/journey-to-event-driven-part-3-affinity-between-events-streams-serverless.html:text/html},
}

@inproceedings{shen_defuse_2021,
	title = {Defuse: A Dependency-Guided Function Scheduler to Mitigate Cold Starts on {FaaS} Platforms},
	doi = {10.1109/ICDCS51616.2021.00027},
	shorttitle = {Defuse},
	abstract = {Function-as-a-Service ({FaaS}) is becoming a prevalent paradigm in developing cloud applications. With {FaaS}, clients can develop applications as serverless functions, leaving the burden of resource management to cloud providers. However, {FaaS} platforms suffer from the performance degradation caused by the cold starts of serverless functions. Cold starts happen when serverless functions are invoked before they have been loaded into the memory. The problem is unavoidable because the memory in datacenters is typically too limited to hold all serverless functions simultaneously. The latency of cold function invocations will greatly degenerate the performance of {FaaS} platforms. Currently, {FaaS} platforms employ various scheduling methods to reduce the occurrences of cold starts. However, they do not consider the ubiquitous dependencies between serverless functions. Observing the potential of using dependencies to mitigate cold starts, we propose Defuse, a Dependency-guided Function Scheduler on {FaaS} platforms. Specifically, Defuse identifies two types of dependencies between serverless functions, i.e., strong dependencies and weak ones. It uses frequent pattern mining and positive point-wise mutual information to mine such dependencies respectively from function invocation histories. In this way, Defuse constructs a function dependency graph. The connected components (i.e., dependent functions) on the graph can be scheduled to diminish the occurrences of cold starts. We evaluate the effectiveness of Defuse by applying it to an industrial serverless dataset. The experimental results show that Defuse can reduce 22\% of memory usage while having a 35\% decrease in function cold-start rates compared with the state-of-the-art method.},
	eventtitle = {2021 {IEEE} 41st International Conference on Distributed Computing Systems ({ICDCS})},
	pages = {194--204},
	booktitle = {2021 {IEEE} 41st International Conference on Distributed Computing Systems ({ICDCS})},
	author = {Shen, Jiacheng and Yang, Tianyi and Su, Yuxin and Zhou, Yangfan and Lyu, Michael R.},
	date = {2021-07},
	note = {{ISSN}: 2575-8411},
	keywords = {Cloud Computing, Cold Start, {FAA}, {FaaS}, Job shop scheduling, Memory management, Processor scheduling, Resource management, Schedules, Serverless, Service Dependency, System improvement},
	file = {IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/FDWXEH7Q/9546470.html:text/html},
}

@inproceedings{karhula_checkpointing_2019,
	location = {New York, {NY}, {USA}},
	title = {Checkpointing and Migration of {IoT} Edge Functions},
	isbn = {978-1-4503-6275-7},
	url = {https://doi.org/10.1145/3301418.3313947},
	doi = {10.1145/3301418.3313947},
	series = {{EdgeSys} '19},
	abstract = {The serverless and functions as a service ({FaaS}) paradigms are currently trending among cloud providers and are now increasingly being applied to the network edge, and to the Internet of Things ({IoT}) devices. The benefits include reduced latency for communication, less network traffic and increased privacy for data processing. However, there are challenges as {IoT} devices have limited resources for running multiple simultaneous containerized functions, and also {FaaS} does not typically support long-running functions. Our implementation utilizes Docker and {CRIU} for checkpointing and suspending long-running blocking functions. The results show that checkpointing is slightly slower than regular Docker pause, but it saves memory and allows for more long-running functions to be run on an {IoT} device. Furthermore, the resulting checkpoint files are small, hence they are suitable for live migration and backing up stateful functions, therefore improving availability and reliability of the system.},
	pages = {60--65},
	booktitle = {Proceedings of the 2nd International Workshop on Edge Systems, Analytics and Networking},
	publisher = {Association for Computing Machinery},
	author = {Karhula, Pekka and Janak, Jan and Schulzrinne, Henning},
	urldate = {2021-11-15},
	date = {2019-03-25},
	keywords = {serverless, Internet of Things, function as a service, checkpointing, light-weight virtualization},
	file = {Full Text PDF:/home/volodia/Zotero/storage/T2D8BJ6H/Karhula et al. - 2019 - Checkpointing and Migration of IoT Edge Functions.pdf:application/pdf;Checkpointing and Migration of IoT Edge Functions:/home/volodia/Zotero/storage/UQ2FL4PU/karhula2019.pdf.pdf:application/pdf},
}

@inproceedings{pinto_dynamic_2018,
	title = {Dynamic Allocation of Serverless Functions in {IoT} Environments},
	doi = {10.1109/EUC.2018.00008},
	abstract = {The {IoT} area has grown significantly in the last few years and is expected to reach a gigantic amount of 50 billion devices by 2020. The appearance of serverless architectures, specifically highlighting {FaaS}, raises the question of the suitability of using them in {IoT} environments. Combining {IoT} with a serverless architectural design can effective when trying to make use of local processing power that exists in a local network of {IoT} devices and creating a fog layer that leverages computational capabilities that are closer to the end-user. In this approach, which is placed between the device and the serverless function, when a device requests for the execution of a serverless function will decide based on previous metrics of execution if the serverless function should be executed locally, in the fog layer of a local network of {IoT} devices, or if it should be executed remotely, in one of the available cloud servers. Therefore, this approach allows dynamically allocating functions to the most suitable layer.},
	eventtitle = {2018 {IEEE} 16th International Conference on Embedded and Ubiquitous Computing ({EUC})},
	pages = {1--8},
	booktitle = {2018 {IEEE} 16th International Conference on Embedded and Ubiquitous Computing ({EUC})},
	author = {Pinto, Duarte and Dias, João Pedro and Sereno Ferreira, Hugo},
	date = {2018-10},
	keywords = {Cloud computing, Edge computing, Fog Computing, Serverless, Internet of Things, Servers, Estimation, Market research, Multi Armed Bandit, Runtime environment, Ubiquitous Computing},
	file = {IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/XSALMCEF/8588841.html:text/html;Dynamic Allocation of Serverless Functions in IoT Environments:/home/volodia/Zotero/storage/RW7YUBWS/pinto2018.pdf.pdf:application/pdf},
}

@inproceedings{cheng_fog_2019,
	title = {Fog Function: Serverless Fog Computing for Data Intensive {IoT} Services},
	doi = {10.1109/SCC.2019.00018},
	shorttitle = {Fog Function},
	abstract = {Fog computing can support {IoT} services with fast response time and low bandwidth usage by moving computation from the cloud to edge devices. However, existing fog computing frameworks have limited flexibility to support dynamic service composition with a data-oriented approach. Function-as-a-Service ({FaaS}) is a promising programming model for fog computing to enhance flexibility, but the current event-or topic-based design of function triggering and the separation of data management and function execution result in inefficiency for data-intensive {IoT} services. To achieve both flexibility and efficiency, we propose a data-centric programming model called Fog Function and also introduce its underlying orchestration mechanism that leverages three types of contexts: data context, system context, and usage context. Moreover, we showcase a concrete use case for smart parking where Fog Function allows service developers to easily model their service logic with reduced learning efforts compared to a static service topology. Our performance evaluation results show that the Fog Function can be scaled to hundreds of fog nodes. Fog Function can improve system efficiency by saving 95\% of the internal data traffic over cloud function and it can reduce service latency by 30\% over edge function.},
	eventtitle = {2019 {IEEE} International Conference on Services Computing ({SCC})},
	pages = {28--35},
	booktitle = {2019 {IEEE} International Conference on Services Computing ({SCC})},
	author = {Cheng, Bin and Fuerst, Jonathan and Solmaz, Gurkan and Sanada, Takuya},
	date = {2019-07},
	note = {{ISSN}: 2474-2473},
	keywords = {fog computing, serverless computing, edge computing, context driven, {IoT} services, programming model},
	file = {IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/BBNBUBAH/8814084.html:text/html;Fog Function\: Serverless Fog Computing for Data Intensive IoT Services:/home/volodia/Zotero/storage/34BHDU36/10.1109@SCC.2019.00018.pdf.pdf:application/pdf},
}

@inproceedings{shen_defuse_2021-1,
	title = {Defuse: A Dependency-Guided Function Scheduler to Mitigate Cold Starts on {FaaS} Platforms},
	doi = {10.1109/ICDCS51616.2021.00027},
	shorttitle = {Defuse},
	abstract = {Function-as-a-Service ({FaaS}) is becoming a prevalent paradigm in developing cloud applications. With {FaaS}, clients can develop applications as serverless functions, leaving the burden of resource management to cloud providers. However, {FaaS} platforms suffer from the performance degradation caused by the cold starts of serverless functions. Cold starts happen when serverless functions are invoked before they have been loaded into the memory. The problem is unavoidable because the memory in datacenters is typically too limited to hold all serverless functions simultaneously. The latency of cold function invocations will greatly degenerate the performance of {FaaS} platforms. Currently, {FaaS} platforms employ various scheduling methods to reduce the occurrences of cold starts. However, they do not consider the ubiquitous dependencies between serverless functions. Observing the potential of using dependencies to mitigate cold starts, we propose Defuse, a Dependency-guided Function Scheduler on {FaaS} platforms. Specifically, Defuse identifies two types of dependencies between serverless functions, i.e., strong dependencies and weak ones. It uses frequent pattern mining and positive point-wise mutual information to mine such dependencies respectively from function invocation histories. In this way, Defuse constructs a function dependency graph. The connected components (i.e., dependent functions) on the graph can be scheduled to diminish the occurrences of cold starts. We evaluate the effectiveness of Defuse by applying it to an industrial serverless dataset. The experimental results show that Defuse can reduce 22\% of memory usage while having a 35\% decrease in function cold-start rates compared with the state-of-the-art method.},
	eventtitle = {2021 {IEEE} 41st International Conference on Distributed Computing Systems ({ICDCS})},
	pages = {194--204},
	booktitle = {2021 {IEEE} 41st International Conference on Distributed Computing Systems ({ICDCS})},
	author = {Shen, Jiacheng and Yang, Tianyi and Su, Yuxin and Zhou, Yangfan and Lyu, Michael R.},
	date = {2021-07},
	note = {{ISSN}: 2575-8411},
	keywords = {Cloud Computing, Cold Start, {FAA}, {FaaS}, Job shop scheduling, Memory management, Processor scheduling, Resource management, Schedules, Serverless, Service Dependency, System improvement},
	file = {IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/YLB5TX68/9546470.html:text/html;IEEE Xplore Full Text PDF:/home/volodia/Zotero/storage/SZJYH9T9/Shen et al. - 2021 - Defuse A Dependency-Guided Function Scheduler to .pdf:application/pdf},
}

@inproceedings{shahrad_architectural_2019,
	location = {New York, {NY}, {USA}},
	title = {Architectural Implications of Function-as-a-Service Computing},
	isbn = {978-1-4503-6938-1},
	url = {https://doi.org/10.1145/3352460.3358296},
	doi = {10.1145/3352460.3358296},
	series = {{MICRO} '52},
	abstract = {Serverless computing is a rapidly growing cloud application model, popularized by Amazon's Lambda platform. Serverless cloud services provide fine-grained provisioning of resources, which scale automatically with user demand. Function-as-a-Service ({FaaS}) applications follow this serverless model, with the developer providing their application as a set of functions which are executed in response to a user- or system-generated event. Functions are designed to be short-lived and execute inside containers or virtual machines, introducing a range of system-level overheads. This paper studies the architectural implications of this emerging paradigm. Using the commercial-grade Apache {OpenWhisk} {FaaS} platform on real servers, this work investigates and identifies the architectural implications of {FaaS} serverless computing. The workloads, along with the way that {FaaS} inherently interleaves short functions from many tenants frustrates many of the locality-preserving architectural structures common in modern processors. In particular, we find that: {FaaS} containerization brings up to 20x slowdown compared to native execution, cold-start can be over 10x a short function's execution time, branch mispredictions per kilo-instruction are 20x higher for short functions, memory bandwidth increases by 6x due to the invocation pattern, and {IPC} decreases by as much as 35\% due to inter-function interference. We open-source {FaaSProfiler}, the {FaaS} testing and profiling platform that we developed for this work.},
	pages = {1063--1075},
	booktitle = {Proceedings of the 52nd Annual {IEEE}/{ACM} International Symposium on Microarchitecture},
	publisher = {Association for Computing Machinery},
	author = {Shahrad, Mohammad and Balkind, Jonathan and Wentzlaff, David},
	urldate = {2021-12-01},
	date = {2019-10-12},
	keywords = {architecture, cloud, faas, function-as-a-service, {OpenWhisk}, serverless},
	file = {Full Text PDF:/home/volodia/Zotero/storage/VIRJR23V/Shahrad et al. - 2019 - Architectural Implications of Function-as-a-Servic.pdf:application/pdf;Architectural Implications of Function-as-a-Service Computing:/home/volodia/Zotero/storage/4BXZ83YP/shahrad2019.pdf.pdf:application/pdf},
}

@online{qiu_is_2021,
	title = {Is Function-as-a-Service a Good Fit for Latency-Critical Services?},
	url = {https://haoran-qiu.com/publication/wosc-2021/},
	abstract = {Function-as-a-Service ({FaaS}) is becoming an increasingly popular cloud-deployment paradigm for serverless computing that frees application developers from managing the infrastructure. At the same time, it allows cloud providers to assert control in workload consolidation, i.e., co-locating multiple containers on the same server, thereby achieving higher server utilization, often at the cost of higher end-toend function request latency. Interestingly, a key aspect of serverless latency management has not been well studied: the trade-off between application developers’ latency goals and the {FaaS} providers’ utilization goals. This paper presents a multi-faceted, measurement-driven study of latency variation in serverless platforms that elucidates this trade-off space. We obtained production measurements by executing {FaaS} benchmarks on {IBM} Cloud and a private cloud to study the impact of workload consolidation, queuing delay, and cold starts on the end-to-end function request latency. We draw several conclusions. For example, increasing a container’s allocated memory limit from 128 {MB} to 256 {MB} reduces the tail latency by 2x but has 1.75x higher power consumption and 59\% lower {CPU} utilization.},
	titleaddon = {Haoran Qiu @Illinois {CS}},
	author = {Qiu, Haoran},
	urldate = {2021-12-01},
	date = {2021-11-19},
	langid = {english},
	file = {Snapshot:/home/volodia/Zotero/storage/NHQU52LE/wosc-2021.html:text/html},
}

@inproceedings{kaffes_centralized_2019,
	location = {New York, {NY}, {USA}},
	title = {Centralized Core-granular Scheduling for Serverless Functions},
	isbn = {978-1-4503-6973-2},
	url = {https://doi.org/10.1145/3357223.3362709},
	doi = {10.1145/3357223.3362709},
	series = {{SoCC} '19},
	abstract = {In recent years, many applications have started using serverless computing platforms primarily due to the ease of deployment and cost efficiency they offer. However, the existing scheduling mechanisms of serverless platforms fall short in catering to the unique characteristics of such applications: burstiness, short and variable execution times, statelessness and use of a single core. Specifically, the existing mechanisms fall short in meeting the requirements generated due to the combined effect of these characteristics: scheduling at a scale of millions of function invocations per second while achieving predictable performance. In this paper, we argue for a cluster-level centralized and core-granular scheduler for serverless functions. By maintaining a global view of the cluster resources, the centralized approach eliminates queue imbalances while the core granularity reduces interference; together these properties enable reduced performance variability. We expect such a scheduler to increase the adoption of serverless computing platforms by various latency and throughput sensitive applications.},
	pages = {158--164},
	booktitle = {Proceedings of the {ACM} Symposium on Cloud Computing},
	publisher = {Association for Computing Machinery},
	author = {Kaffes, Kostis and Yadwadkar, Neeraja J. and Kozyrakis, Christos},
	urldate = {2021-12-01},
	date = {2019-11-20},
	keywords = {cloud computing, resource allocation, scheduling, serverless computing},
	file = {Centralized Core-granular Scheduling for Serverless Functions:/home/volodia/Zotero/storage/B8RPESQN/kaffes2019.pdf.pdf:application/pdf},
}

@inproceedings{palade_swarm-based_2020,
	title = {A Swarm-based Approach for Function Placement in Federated Edges},
	doi = {10.1109/SCC49832.2020.00013},
	abstract = {Multi-access Edge Computing ({MEC}) provides cloud computing capabilities at the edge by offloading users' service requests on {MEC} servers deployed at Base Stations ({BS}). Optimising the resource allocation on such distributed units in a physical area such as a city, especially for compute-intensive and latency-critical services, is a key challenge. We propose a swarm-based approach for placing functions in the edge using a serverless architecture, which does not require services to pre-occupy the required computing resources. The approach uses a probabilistic model to decide where to place the functions while considering the resources available at each {MEC} server and the latency between the physical servers and the application requester. A central controller with a federated view of available {MEC} servers orchestrates functions' deployment and deals changes available resources. We compare our approach against the Best-Fit, Max-Fit, {MultiOpt}, {ILP} and Random baselines. Results show that our approach can reduce the latency of applications with limited effect on the resource utilisation.},
	eventtitle = {2020 {IEEE} International Conference on Services Computing ({SCC})},
	pages = {48--50},
	booktitle = {2020 {IEEE} International Conference on Services Computing ({SCC})},
	author = {Palade, Andrei and Mukhopadhyay, Atri and Kazmi, Aqeel and Cabrera, Christian and Nomayo, Evelyn and Iosifidis, Georgios and Ruffini, Marco and Clarke, Siobhán},
	date = {2020-11},
	note = {{ISSN}: 2474-2473},
	keywords = {Edge computing, Edge Computing, Resource management, Serverless, Servers, Conferences, Probabilistic logic, Service computing, Service Placement, Urban areas},
	file = {IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/PSZSEFH6/9284644.html:text/html;A Swarm-based Approach for Function Placement in Federated Edges:/home/volodia/Zotero/storage/PXMX7SRX/palade2020.pdf.pdf:application/pdf},
}

@inproceedings{george_nanolambda_2020,
	location = {San Jose, {CA}, {USA}},
	title = {{NanoLambda}: Implementing Functions as a Service at All Resource Scales for the Internet of Things.},
	isbn = {978-1-72815-943-0},
	url = {https://ieeexplore.ieee.org/document/9355717/},
	doi = {10.1109/SEC50012.2020.00035},
	shorttitle = {{NanoLambda}},
	abstract = {Internet of Things ({IoT}) devices are becoming increasingly prevalent in our environment, yet the process of programming these devices and processing the data they produce remains difﬁcult. Typically, data is processed on device, involving arduous work in low level languages, or data is moved to the cloud, where abundant resources are available for Functions as a Service ({FaaS}) or other handlers. {FaaS} is an emerging category of ﬂexible computing services, where developers deploy self-contained functions to be run in portable and secure containerized environments; however, at the moment, these functions are limited to running in the cloud or in some cases at the “edge” of the network using resource rich, Linux-based systems.},
	eventtitle = {2020 {IEEE}/{ACM} Symposium on Edge Computing ({SEC})},
	pages = {220--231},
	booktitle = {2020 {IEEE}/{ACM} Symposium on Edge Computing ({SEC})},
	publisher = {{IEEE}},
	author = {George, Gareth and Bakir, Fatih and Wolski, Rich and Krintz, Chandra},
	urldate = {2021-11-15},
	date = {2020-11},
	langid = {english},
	file = {George et al. - 2020 - NanoLambda Implementing Functions as a Service at.pdf:/home/volodia/Zotero/storage/Q9T7BFNY/George et al. - 2020 - NanoLambda Implementing Functions as a Service at.pdf:application/pdf;NanoLambda\: Implementing Functions as a Service at All Resource Scales for the Internet of Things.:/home/volodia/Zotero/storage/IM7QKEP5/george2020.pdf.pdf:application/pdf},
}

@inproceedings{elgamal_costless_2018,
	title = {Costless: Optimizing Cost of Serverless Computing through Function Fusion and Placement},
	doi = {10.1109/SEC.2018.00029},
	shorttitle = {Costless},
	abstract = {Serverless computing has recently experienced significant adoption by several applications, especially Internet of Things ({IoT}) applications. In serverless computing, rather than deploying and managing dedicated virtual machines, users are able to deploy individual functions, and pay only for the time that their code is actually executing. However, since serverless platforms are relatively new, they have a completely different pricing model that depends on the memory, duration, and the number of executions of a sequence/workflow of functions. In this paper we present an algorithm that optimizes the price of serverless applications in {AWS} Lambda. We first describe the factors affecting price of serverless applications which include: (1) fusing a sequence of functions, (2) splitting functions across edge and cloud resources, and (3) allocating the memory for each function. We then present an efficient algorithm to explore different function fusion-placement solutions and find the solution that optimizes the application's price while keeping the latency under a certain threshold. Our results on image processing workflows show that the algorithm can find solutions optimizing the price by more than 35\%-57\% with only 5\%-15\% increase in latency. We also show that our algorithm can find non-trivial memory configurations that reduce both latency and price.},
	eventtitle = {2018 {IEEE}/{ACM} Symposium on Edge Computing ({SEC})},
	pages = {300--312},
	booktitle = {2018 {IEEE}/{ACM} Symposium on Edge Computing ({SEC})},
	author = {Elgamal, Tarek},
	date = {2018-10},
	keywords = {Cloud computing, Cloud Computing, Edge computing, Memory management, Serverless, Computational modeling, Internet of Things, Pricing, {AWS} Lambda, Cost Optimization, Face, Fuses},
	file = {IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/GN8FJLJA/8567674.html:text/html;Submitted Version:/home/volodia/Zotero/storage/3HQKGIJJ/Elgamal - 2018 - Costless Optimizing Cost of Serverless Computing .pdf:application/pdf;Costless\: Optimizing Cost of Serverless Computing through Function Fusion and Placement:/home/volodia/Zotero/storage/QWEZ4724/df7ac33bf08b28a3b6cfdd661c3b41c4.pdf.pdf:application/pdf},
}

@article{rausch_optimized_2021,
	title = {Optimized container scheduling for data-intensive serverless edge computing},
	volume = {114},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X2030399X},
	doi = {10.1016/j.future.2020.07.017},
	abstract = {Operating data-intensive applications on edge systems is challenging, due to the extreme workload and device heterogeneity, as well as the geographic dispersion of compute and storage infrastructure. Serverless computing has emerged as a compelling model to manage the complexity of such systems, by decoupling the underlying infrastructure and scaling mechanisms from applications. Although serverless platforms have reached a high level of maturity, we have found several limiting factors that inhibit their use in an edge setting. This paper presents a container scheduling system that enables such platforms to make efficient use of edge infrastructures. Our scheduler makes heuristic trade-offs between data and computation movement, and considers workload-specific compute requirements such as {GPU} acceleration. Furthermore, we present a method to automatically fine-tune the weights of scheduling constraints to optimize high-level operational objectives such as minimizing task execution time, uplink usage, or cloud execution cost. We implement a prototype that targets the container orchestration system Kubernetes, and deploy it on an edge testbed we have built. We evaluate our system with trace-driven simulations in different infrastructure scenarios, using traces generated from running representative workloads on our testbed. Our results show that (a) our scheduler significantly improves the quality of task placement compared to the state-of-the-art scheduler of Kubernetes, and (b) our method for fine-tuning scheduling parameters helps significantly in meeting operational goals.},
	pages = {259--271},
	journaltitle = {Future Generation Computer Systems},
	shortjournal = {Future Generation Computer Systems},
	author = {Rausch, Thomas and Rashed, Alexander and Dustdar, Schahram},
	urldate = {2021-11-15},
	date = {2021-01-01},
	langid = {english},
	keywords = {Edge computing, Serverless, Container scheduling, Machine learning},
	file = {Optimized container scheduling for data-intensive serverless edge computing:/home/volodia/Zotero/storage/PUMJ9WFQ/rausch2021.pdf.pdf:application/pdf},
}

@article{das_performance_2020,
	title = {Performance Optimization for Edge-Cloud Serverless Platforms via Dynamic Task Placement},
	url = {http://arxiv.org/abs/2003.01310},
	abstract = {We present a framework for performance optimization in serverless edge-cloud platforms using dynamic task placement. We focus on applications for smart edge devices, for example, smart cameras or speakers, that need to perform processing tasks on input data in real to near-real time. Our framework allows the user to specify cost and latency requirements for each application task, and for each input, it determines whether to execute the task on the edge device or in the cloud. Further, for cloud executions, the framework identifies the container resource configuration needed to satisfy the performance goals. We have evaluated our framework in simulation using measurements collected from serverless applications in {AWS} Lambda and {AWS} Greengrass. In addition, we have implemented a prototype of our framework that runs in these same platforms. In experiments with our prototype, our models can predict average end-to-end latency with less than 6\% error, and we obtain almost three orders of magnitude reduction in end-to-end latency compared to edge-only execution.},
	journaltitle = {{arXiv}:2003.01310 [cs]},
	author = {Das, Anirban and Imai, Shigeru and Wittie, Mike P. and Patterson, Stacy},
	urldate = {2021-11-15},
	date = {2020-05-19},
	eprinttype = {arxiv},
	eprint = {2003.01310},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Networking and Internet Architecture},
	file = {arXiv.org Snapshot:/home/volodia/Zotero/storage/FHPYSBUN/2003.html:text/html;arXiv Fulltext PDF:/home/volodia/Zotero/storage/WSDVZYVS/Das et al. - 2020 - Performance Optimization for Edge-Cloud Serverless.pdf:application/pdf},
}

@article{ascigil_resource_2021,
	title = {Resource Provisioning and Allocation in Function-as-a-Service Edge-Clouds},
	issn = {1939-1374},
	doi = {10.1109/TSC.2021.3052139},
	abstract = {Edge computing has emerged as a new paradigm to bring cloud applications closer to users for increased performance. Unlike back-end cloud systems which consolidate their resources in a centralized data center location with virtually unlimited capacity, edge-clouds comprise distributed resources at various computation spots, each with very limited capacity. In this paper, we consider Function-as-a-Service ({FaaS}) edge-clouds where application providers deploy their latency-critical functions that process user requests with strict response time deadlines. In this setting, we investigate the problem of resource provisioning and allocation. After formulating the optimal solution, we propose resource allocation and provisioning algorithms across the spectrum of fully-centralized to fully-decentralized. We evaluate the performance of these algorithms in terms of their ability to utilize {CPU} resources and meet request deadlines under various system parameters. Our results indicate that practical decentralized strategies, which require no coordination among computation spots, achieve performance that is close to the optimal fully-centralized strategy with coordination overheads.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Services Computing},
	author = {Ascigil, Onur and Tasiopoulos, Argyrios and Phan, Truong Khoa and Sourlas, Vasilis and Psaras, Ioannis and Pavlou, George},
	date = {2021},
	note = {Conference Name: {IEEE} Transactions on Services Computing},
	keywords = {Cloud computing, {FAA}, Resource management, Delays, Containers, Image edge detection, Routing},
	file = {IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/T4AJWKZN/9326369.html:text/html;Full Text:/home/volodia/Zotero/storage/TAGMTENM/Ascigil et al. - 2021 - Resource Provisioning and Allocation in Function-a.pdf:application/pdf;Resource Provisioning and Allocation in Function-as-a-Service Edge-Clouds:/home/volodia/Zotero/storage/TAKJQFRV/ascigil2021.pdf.pdf:application/pdf},
}

@article{wang_lass_2021,
	title = {{LaSS}: Running Latency Sensitive Serverless Computations at the Edge},
	url = {http://arxiv.org/abs/2104.14087},
	doi = {10.1145/3431379.3460646},
	shorttitle = {{LaSS}},
	abstract = {Serverless computing has emerged as a new paradigm for running short-lived computations in the cloud. Due to its ability to handle {IoT} workloads, there has been considerable interest in running serverless functions at the edge. However, the constrained nature of the edge and the latency sensitive nature of workloads result in many challenges for serverless platforms. In this paper, we present {LaSS}, a platform that uses model-driven approaches for running latency-sensitive serverless computations on edge resources. {LaSS} uses principled queuing-based methods to determine an appropriate allocation for each hosted function and auto-scales the allocated resources in response to workload dynamics. {LaSS} uses a fair-share allocation approach to guarantee a minimum of allocated resources to each function in the presence of overload. In addition, it utilizes resource reclamation methods based on container deflation and termination to reassign resources from over-provisioned functions to under-provisioned ones. We implement a prototype of our approach on an {OpenWhisk} serverless edge cluster and conduct a detailed experimental evaluation. Our results show that {LaSS} can accurately predict the resources needed for serverless functions in the presence of highly dynamic workloads, and reprovision container capacity within hundreds of milliseconds while maintaining fair share allocation guarantees.},
	pages = {239--251},
	journaltitle = {Proceedings of the 30th International Symposium on High-Performance Parallel and Distributed Computing},
	author = {Wang, Bin and Ali-Eldin, Ahmed and Shenoy, Prashant},
	urldate = {2021-11-15},
	date = {2021-06-21},
	eprinttype = {arxiv},
	eprint = {2104.14087},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv.org Snapshot:/home/volodia/Zotero/storage/EX2NPTFI/2104.html:text/html;arXiv Fulltext PDF:/home/volodia/Zotero/storage/3F9W2HWB/Wang et al. - 2021 - LaSS Running Latency Sensitive Serverless Computa.pdf:application/pdf;LaSS\: Running Latency Sensitive Serverless Computations at the Edge:/home/volodia/Zotero/storage/2I46FE3Q/wang2020.pdf.pdf:application/pdf},
}

@article{puliafito_stateful_2021,
	title = {Stateful Function-as-a-Service at the Edge},
	url = {http://arxiv.org/abs/2109.15040},
	abstract = {In {FaaS}, users invoke remote functions, which encapsulate service(s). These functions typically need to remotely access a persistent state via external services: this makes the paradigm less attractive in edge systems, especially for {IoT} applications, due to the increased delay and outbound traffic. We propose to generalize the {FaaS} paradigm by allowing functions to alternate between remote-state and local-state phases, depending on internal and external conditions, and dedicating a container with persistent memory to functions when in a local-state phase. We present initial results showing that this simple yet powerful pattern allows to better utilize the available resources, which are scarce on edge nodes, while significantly reducing tail latencies, which is key to enable many new applications based on real-time {ML}, e.g., in smart vehicles and smart factory scenarios},
	journaltitle = {{arXiv}:2109.15040 [cs]},
	author = {Puliafito, Carlo and Cicconetti, Claudio and Conti, Marco and Mingozzi, Enzo and Passarella, Andrea},
	urldate = {2021-12-01},
	date = {2021-09-30},
	eprinttype = {arxiv},
	eprint = {2109.15040},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Networking and Internet Architecture},
	file = {arXiv.org Snapshot:/home/volodia/Zotero/storage/2JFJXVYC/2109.html:text/html;arXiv Fulltext PDF:/home/volodia/Zotero/storage/MCBLNV7M/Puliafito et al. - 2021 - Stateful Function-as-a-Service at the Edge.pdf:application/pdf},
}

@inproceedings{pfandzelter_tinyfaas_2020,
	title = {{tinyFaaS}: A Lightweight {FaaS} Platform for Edge Environments},
	doi = {10.1109/ICFC49376.2020.00011},
	shorttitle = {{tinyFaaS}},
	abstract = {The Function-as-a-Service ({FaaS}) model is a great fit for data and event processing in the Internet of Things ({IoT}). Sending all data to a cloud-based {FaaS} platform, however, may cause performance and privacy issues. While these issues could be mitigated using edge computing, existing {FaaS} approaches, designed for the cloud, are too heavyweight to run on small, constrained edge nodes. In this paper, we propose {tinyFaaS}, a new {FaaS} system that is specifically designed for edge environments and their unique challenges. Our platform is lightweight enough to run on low-performance single machine edge nodes, provides a {CoAP} endpoint to support communication with low-power devices, and uses Docker containers to isolate tenants. We evaluate {tinyFaaS} through a proof-of-concept implementation that we benchmark and compare to state-of-the-art {FaaS} platforms. For {IoT} processing scenarios, we find that {tinyFaaS} outperforms existing systems by at least an order of magnitude.},
	eventtitle = {2020 {IEEE} International Conference on Fog Computing ({ICFC})},
	pages = {17--24},
	booktitle = {2020 {IEEE} International Conference on Fog Computing ({ICFC})},
	author = {Pfandzelter, Tobias and Bermbach, David},
	date = {2020-04},
	keywords = {Cloud computing, Edge computing, Edge Computing, {IoT}, {FAA}, {FaaS}, Serverless, Servers, Runtime, Containers, Protocols},
	file = {IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/FF4IUIKL/9103476.html:text/html;tinyFaaS\: A Lightweight FaaS Platform for Edge Environments:/home/volodia/Zotero/storage/QNQPJXS2/pfandzelter2020.pdf.pdf:application/pdf},
}

@online{breitgand_lean_2018,
	title = {Lean {OpenWhisk}: Open Source {FaaS} for Edge Computing},
	url = {https://medium.com/openwhisk/lean-openwhisk-open-source-faas-for-edge-computing-fb823c6bbb9b},
	shorttitle = {Lean {OpenWhisk}},
	abstract = {This Blog is co-authored with Pavel Kravchenko as part of our work at {IBM} Research — Haifa*.},
	titleaddon = {Apache {OpenWhisk}},
	author = {Breitgand, David},
	urldate = {2021-11-15},
	date = {2018-07-25},
	langid = {english},
	file = {Snapshot:/home/volodia/Zotero/storage/893I5YG7/lean-openwhisk-open-source-faas-for-edge-computing-fb823c6bbb9b.html:text/html},
}

@software{noauthor_previous_2021,
	title = {Previous Serverless Version 0.5.x},
	rights = {{MIT}},
	url = {https://github.com/serverless/serverless},
	abstract = {⚡ Serverless Framework – Build web, mobile and {IoT} applications with serverless architectures using {AWS} Lambda, Azure Functions, Google {CloudFunctions} \& more! –},
	publisher = {Serverless},
	urldate = {2021-11-15},
	date = {2021-11-15},
	note = {original-date: 2015-04-21T03:48:40Z},
	keywords = {serverless, aws, aws-dynamodb, aws-lambda, azure-functions, google-cloud-functions, microservice, serverless-architectures, serverless-framework},
}

@software{noauthor_firecracker-microvmfirecracker_2021,
	title = {firecracker-microvm/firecracker},
	rights = {Apache-2.0},
	url = {https://github.com/firecracker-microvm/firecracker},
	abstract = {Secure and fast {microVMs} for serverless computing.},
	publisher = {firecracker-microvm},
	urldate = {2021-11-15},
	date = {2021-11-15},
	note = {original-date: 2017-10-19T06:18:47Z},
	keywords = {serverless, containers, minimalist, open-source, oversubscription, rust, sandbox, virtual-machine, virtualization},
}

@software{noauthor_slackhqnebula_2021,
	title = {slackhq/nebula},
	rights = {{MIT}},
	url = {https://github.com/slackhq/nebula},
	abstract = {A scalable overlay networking tool with a focus on performance, simplicity and security},
	publisher = {Slack},
	urldate = {2021-11-15},
	date = {2021-11-15},
	note = {original-date: 2019-11-16T23:26:23Z},
}

@article{lee_trustful_2020,
	title = {Trustful Resource Management for Service Allocation in Fog-Enabled Intelligent Transportation Systems},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3015550},
	abstract = {A fog-enabled intelligent transportation system is constructed using Vehicle-to-Everything (V2X) communications towards the Internet of Vehicles ({IoV}). In this system, the road side unit ({RSU}) and vehicle correspond to the fog server and fog device, respectively. In practice, the {RSU} offers instant infotainment services (e.g., video streaming and traffic assistance) to vehicles. However, the vehicles are considered untrustworthy and could possibly be exploited to attack the system, thereby compromising the service stability and data integrity. To address this problem, this article proposes truthful service allocation using a Vickrey-Clarke-Groves ({VCG}) auction mechanism. The {RSU} leads the proposed auction as a seller who sells their computing resources, and the vehicles participate as buyers who purchase these computing resources for the offered services. Consequently, the {RSU} generates a transaction whenever a service allocation is assigned. To secure the service transactions, we utilize a distributed blockchain system that implements Hyperledger Fabric framework among {RSUs} for transaction verification. The simulation results demonstrate that the proposed system provides service stability while ensuring service trustfulness.},
	pages = {147313--147322},
	journaltitle = {{IEEE} Access},
	author = {Lee, Yunseong and Jeong, Seohyeon and Masood, Arooj and Park, Laihyuk and Dao, Nhu-Ngoc and Cho, Sungrae},
	date = {2020},
	note = {Conference Name: {IEEE} Access},
	keywords = {Cloud computing, Computer architecture, Edge computing, Resource management, fog computing, Data centers, Servers, Blockchain, Internet of Vehicles ({IoV}), {VCG}-auction},
	file = {IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/ARYFYZBH/9163371.html:text/html;IEEE Xplore Full Text PDF:/home/volodia/Zotero/storage/Y3NATAWP/Lee et al. - 2020 - Trustful Resource Management for Service Allocatio.pdf:application/pdf;Trustful Resource Management for Service Allocation in Fog-Enabled Intelligent Transportation Systems:/home/volodia/Zotero/storage/XJZCQBJ2/lee2020.pdf.pdf:application/pdf},
}

@article{kjorveziroski_iot_2021,
	title = {{IoT} Serverless Computing at the Edge: A Systematic Mapping Review},
	volume = {10},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2073-431X/10/10/130},
	doi = {10.3390/computers10100130},
	shorttitle = {{IoT} Serverless Computing at the Edge},
	abstract = {Serverless computing is a new concept allowing developers to focus on the core functionality of their code, while abstracting away the underlying infrastructure. Even though there are existing commercial serverless cloud providers and open-source solutions, dealing with the explosive growth of new Internet of Things ({IoT}) devices requires more efficient bandwidth utilization, reduced latency, and data preprocessing closer to the source, thus reducing the overall data volume and meeting privacy regulations. Moving serverless computing to the edge of the network is a topic that is actively being researched with the aim of solving these issues. This study presents a systematic mapping review of current progress made to this effect, analyzing work published between 1 January 2015 and 1 September 2021. Using a document selection methodology which emphasizes the quality of the papers obtained through querying several popular databases with relevant search terms, we have included 64 entries, which we then further categorized into eight main categories. Results show that there is an increasing interest in this area with rapid progress being made to solve the remaining open issues, which have also been summarized in this paper. Special attention is paid to open-source efforts, as well as open-access contributions.},
	pages = {130},
	number = {10},
	journaltitle = {Computers},
	author = {Kjorveziroski, Vojdan and Filiposka, Sonja and Trajkovik, Vladimir},
	urldate = {2021-11-30},
	date = {2021-10},
	langid = {english},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {serverless computing, Internet of Things, edge computing, function as a service, systematic review},
	file = {Snapshot:/home/volodia/Zotero/storage/BPMH6S7U/130.html:text/html;Full Text PDF:/home/volodia/Zotero/storage/HCK9EVGW/Kjorveziroski et al. - 2021 - IoT Serverless Computing at the Edge A Systematic.pdf:application/pdf},
}

@article{hassan_survey_2021,
	title = {Survey on serverless computing},
	volume = {10},
	issn = {2192-113X},
	url = {https://doi.org/10.1186/s13677-021-00253-7},
	doi = {10.1186/s13677-021-00253-7},
	abstract = {Serverless computing has gained importance over the last decade as an exciting new field, owing to its large influence in reducing costs, decreasing latency, improving scalability, and eliminating server-side management, to name a few. However, to date there is a lack of in-depth survey that would help developers and researchers better understand the significance of serverless computing in different contexts. Thus, it is essential to present research evidence that has been published in this area. In this systematic survey, 275 research papers that examined serverless computing from well-known literature databases were extensively reviewed to extract useful data. Then, the obtained data were analyzed to answer several research questions regarding state-of-the-art contributions of serverless computing, its concepts, its platforms, its usage, etc. We moreover discuss the challenges that serverless computing faces nowadays and how future research could enable its implementation and usage.},
	pages = {39},
	number = {1},
	journaltitle = {Journal of Cloud Computing},
	shortjournal = {Journal of Cloud Computing},
	author = {Hassan, Hassan B. and Barakat, Saman A. and Sarhan, Qusay I.},
	urldate = {2021-11-29},
	date = {2021-07-12},
	keywords = {Cloud computing, Serverless benefits, Serverless challenges, Serverless computing, Serverless platforms, Survey},
	file = {Snapshot:/home/volodia/Zotero/storage/6AD7D2WK/s13677-021-00253-7.html:text/html;Full Text PDF:/home/volodia/Zotero/storage/VDEYH4WR/Hassan et al. - 2021 - Survey on serverless computing.pdf:application/pdf},
}

@online{noauthor_fission_nodate,
	title = {Fission},
	url = {https://fission.io/},
	abstract = {Fast Opensource Kubernetes Serverless Framework},
	titleaddon = {Fission},
	urldate = {2021-11-16},
	langid = {english},
	file = {Snapshot:/home/volodia/Zotero/storage/GKUW3EA5/fission.io.html:text/html},
}

@inproceedings{glikson_deviceless_2017,
	location = {New York, {NY}, {USA}},
	title = {Deviceless edge computing: extending serverless computing to the edge of the network},
	isbn = {978-1-4503-5035-8},
	url = {https://doi.org/10.1145/3078468.3078497},
	doi = {10.1145/3078468.3078497},
	series = {{SYSTOR} '17},
	shorttitle = {Deviceless edge computing},
	abstract = {The serverless paradigm has been rapidly adopted by developers of cloud-native applications, mainly because it relieves them from the burden of provisioning, scaling and operating the underlying infrastructure. In this paper, we propose a novel computing paradigm - Deviceless Edge Computing that extends the serverless paradigm to the edge of the network, enabling {IoT} and Edge devices to be seamlessly integrated as application execution infrastructure. We also discuss open challenges to realize Deviceless Edge Computing, based on our experience in prototyping a deviceless platform.},
	pages = {1},
	booktitle = {Proceedings of the 10th {ACM} International Systems and Storage Conference},
	publisher = {Association for Computing Machinery},
	author = {Glikson, Alex and Nastic, Stefan and Dustdar, Schahram},
	urldate = {2021-11-16},
	date = {2017-05-22},
	keywords = {serverless computing, edge computing, function as a service},
	file = {Deviceless edge computing\: extending serverless computing to the edge of the network:/home/volodia/Zotero/storage/DE3VI26G/5051b39f2a586bb6fd303a6cf790c456.pdf.pdf:application/pdf},
}

@online{noauthor_openzipkin_nodate,
	title = {{OpenZipkin} · A distributed tracing system},
	url = {https://zipkin.io/},
	urldate = {2021-11-16},
	file = {OpenZipkin · A distributed tracing system:/home/volodia/Zotero/storage/WNX6SPUS/zipkin.io.html:text/html},
}

@article{cicconetti_decentralized_2021,
	title = {A Decentralized Framework for Serverless Edge Computing in the Internet of Things},
	volume = {18},
	issn = {1932-4537},
	doi = {10.1109/TNSM.2020.3023305},
	abstract = {Serverless computing is becoming widely adopted among cloud providers, thus making increasingly popular the Function-as-a-Service ({FaaS}) programming model, where the developers realize services by packaging sequences of stateless function calls. The current technologies are very well suited to data centers, but cannot provide equally good performance in decentralized environments, such as edge computing systems, which are expected to be typical for Internet of Things ({IoT}) applications. In this article, we fill this gap by proposing a framework for efficient dispatching of stateless tasks to in-network executors so as to minimize the response times while exhibiting short- and long-term fairness, also leveraging information from a virtualized network infrastructure when available. Our solution is shown to be simple enough to be installed on devices with limited computational capabilities, such as {IoT} gateways, especially when using a hierarchical forwarding extension. We evaluate the proposed platform by means of extensive emulation experiments with a prototype implementation in realistic conditions. The results show that it is able to smoothly adapt to the mobility of clients and to the variations of their service request patterns, while coping promptly with network congestion.},
	pages = {2166--2180},
	number = {2},
	journaltitle = {{IEEE} Transactions on Network and Service Management},
	author = {Cicconetti, Claudio and Conti, Marco and Passarella, Andrea},
	date = {2021-06},
	note = {Conference Name: {IEEE} Transactions on Network and Service Management},
	keywords = {Computer architecture, Edge computing, Computational modeling, computer simulation experiments, Data centers, Internet of Things, Internet of Things services, overlay networks, Peer-to-peer computing, software-defined networking, Task analysis},
	file = {IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/Z3A6JS3P/9193994.html:text/html;Submitted Version:/home/volodia/Zotero/storage/9GPZR6ND/Cicconetti et al. - 2021 - A Decentralized Framework for Serverless Edge Comp.pdf:application/pdf;A Decentralized Framework for Serverless Edge Computing in the Internet of Things:/home/volodia/Zotero/storage/BYBFK4JS/10.1109@TNSM.2020.3023305.pdf.pdf:application/pdf},
}

@article{mutichiro_qos-based_2021,
	title = {{QoS}-Based Service-Time Scheduling in the {IoT}-Edge Cloud},
	volume = {21},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/21/17/5797},
	doi = {10.3390/s21175797},
	abstract = {In edge computing, scheduling heterogeneous workloads with diverse resource requirements is challenging. Besides limited resources, the servers may be overwhelmed with computational tasks, resulting in lengthy task queues and congestion occasioned by unusual network traffic patterns. Additionally, Internet of Things ({IoT})/Edge applications have different characteristics coupled with performance requirements, which become determinants if most edge applications can both satisfy deadlines and each user’s {QoS} requirements. This study aims to address these restrictions by proposing a mechanism that improves the cluster resource utilization and Quality of Service ({QoS}) in an edge cloud cluster in terms of service time. Containerization can provide a way to improve the performance of the {IoT}-Edge cloud by factoring in task dependencies and heterogeneous application resource demands. In this paper, we propose {STaSA}, a service time aware scheduler for the edge environment. The algorithm automatically assigns requests onto different processing nodes and then schedules their execution under real-time constraints, thus minimizing the number of {QoS} violations. The effectiveness of our scheduling model is demonstrated through implementation on {KubeEdge}, a container orchestration platform based on Kubernetes. Experimental results show significantly fewer violations in {QoS} during scheduling and improved performance compared to the state of the art.},
	pages = {5797},
	number = {17},
	journaltitle = {Sensors},
	author = {Mutichiro, Briytone and Tran, Minh-Ngoc and Kim, Young-Han},
	urldate = {2021-12-01},
	date = {2021-01},
	langid = {english},
	note = {Number: 17
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {ant colony optimization ({ACO}), {IoT}-edge cloud, quality of service ({QoS}), resource scheduling},
	file = {Snapshot:/home/volodia/Zotero/storage/6RA2U3HV/5797.html:text/html;Full Text PDF:/home/volodia/Zotero/storage/4E8YEA8N/Mutichiro et al. - 2021 - QoS-Based Service-Time Scheduling in the IoT-Edge .pdf:application/pdf},
}

@article{xie_when_2021,
	title = {When Serverless Computing Meets Edge Computing: Architecture, Challenges, and Open Issues},
	volume = {28},
	issn = {1558-0687},
	doi = {10.1109/MWC.001.2000466},
	shorttitle = {When Serverless Computing Meets Edge Computing},
	abstract = {Edge computing enables applications to leverage computing resources near the data source to perform data processing, which can effectively cope with the explosive growth of network data. However, along with the benefits come great challenges: application developers need to explicitly manage their resources and handle the burden of scalability and load balancing; users have to pay based on the allocated resources, not the resources actually consumed; and so forth. Serverless edge computing extends the idea of serverless computing and has emerged as a compelling model for dealing with many of the challenges associated with edge infrastructure. Different from the existing works which mainly focus on the basic architectures, platforms, and optimization models, this article aims to provide a systematic and comprehensive overview of server-less edge computing networks from the perspective of networking. We first propose the network architecture and layered structure of serverless edge computing networks. Then, the communication process, as well as the implementation and deployment, is presented. Next, the promising technical challenges, including service deployment and lifecycle management, resource awareness and service discovery, service scheduling, and so on, are discussed. Finally, some potential areas for future research are highlighted.},
	pages = {126--133},
	number = {5},
	journaltitle = {{IEEE} Wireless Communications},
	author = {Xie, Renchao and Tang, Qinqin and Qiao, Shi and Zhu, Han and Yu, F. Richard and Huang, Tao},
	date = {2021-10},
	note = {Conference Name: {IEEE} Wireless Communications},
	keywords = {Cloud computing, Computer architecture, Edge computing, Processor scheduling, Resource management, Reliability engineering, Servers},
	file = {IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/PZ9JL9W2/9474932.html:text/html;IEEE Xplore Full Text PDF:/home/volodia/Zotero/storage/NZGGBWSP/Xie et al. - 2021 - When Serverless Computing Meets Edge Computing Ar.pdf:application/pdf},
}

@inproceedings{elgamal_droplet_2018,
	location = {San Francisco, {CA}, {USA}},
	title = {{DROPLET}: Distributed Operator Placement for {IoT} Applications Spanning Edge and Cloud Resources},
	isbn = {978-1-5386-7235-8},
	url = {https://ieeexplore.ieee.org/document/8457776/},
	doi = {10.1109/CLOUD.2018.00008},
	shorttitle = {{DROPLET}},
	eventtitle = {2018 {IEEE} 11th International Conference on Cloud Computing ({CLOUD})},
	pages = {1--8},
	booktitle = {2018 {IEEE} 11th International Conference on Cloud Computing ({CLOUD})},
	publisher = {{IEEE}},
	author = {Elgamal, Tarek and Sandur, Atul and Nguyen, Phuong and Nahrstedt, Klara and Agha, Gul},
	urldate = {2021-11-29},
	date = {2018-07},
	file = {DROPLET\: Distributed Operator Placement for IoT Applications Spanning Edge and Cloud Resources:/home/volodia/Zotero/storage/NLT6JI48/1fed32f79e64180a31f4836fca21058c.pdf.pdf:application/pdf},
}

@inproceedings{karhula_checkpointing_2019-1,
	location = {New York, {NY}, {USA}},
	title = {Checkpointing and Migration of {IoT} Edge Functions},
	isbn = {978-1-4503-6275-7},
	url = {https://doi.org/10.1145/3301418.3313947},
	doi = {10.1145/3301418.3313947},
	series = {{EdgeSys} '19},
	abstract = {The serverless and functions as a service ({FaaS}) paradigms are currently trending among cloud providers and are now increasingly being applied to the network edge, and to the Internet of Things ({IoT}) devices. The benefits include reduced latency for communication, less network traffic and increased privacy for data processing. However, there are challenges as {IoT} devices have limited resources for running multiple simultaneous containerized functions, and also {FaaS} does not typically support long-running functions. Our implementation utilizes Docker and {CRIU} for checkpointing and suspending long-running blocking functions. The results show that checkpointing is slightly slower than regular Docker pause, but it saves memory and allows for more long-running functions to be run on an {IoT} device. Furthermore, the resulting checkpoint files are small, hence they are suitable for live migration and backing up stateful functions, therefore improving availability and reliability of the system.},
	pages = {60--65},
	booktitle = {Proceedings of the 2nd International Workshop on Edge Systems, Analytics and Networking},
	publisher = {Association for Computing Machinery},
	author = {Karhula, Pekka and Janak, Jan and Schulzrinne, Henning},
	urldate = {2021-11-15},
	date = {2019-03-25},
	keywords = {serverless, Internet of Things, function as a service, checkpointing, light-weight virtualization},
	file = {Full Text PDF:/home/volodia/Zotero/storage/F86CD3YK/Karhula et al. - 2019 - Checkpointing and Migration of IoT Edge Functions.pdf:application/pdf;Checkpointing and Migration of IoT Edge Functions:/home/volodia/Zotero/storage/4QJC9CWW/karhula2019.pdf.pdf:application/pdf},
}

@inproceedings{hall_execution_2019,
	location = {New York, {NY}, {USA}},
	title = {An execution model for serverless functions at the edge},
	isbn = {978-1-4503-6283-2},
	url = {https://doi.org/10.1145/3302505.3310084},
	doi = {10.1145/3302505.3310084},
	series = {{IoTDI} '19},
	abstract = {Serverless computing platforms allow developers to host single-purpose applications that automatically scale with demand. In contrast to traditional long-running applications on dedicated, virtualized, or container-based platforms, serverless applications are intended to be instantiated when called, execute a single function, and shut down when finished. State-of-the-art serverless platforms achieve these goals by creating a new container instance to host a function when it is called and destroying the container when it completes. This design allows for cost and resource savings when hosting simple applications, such as those supporting {IoT} devices at the edge of the network. However, the use of containers introduces some overhead which may be unsuitable for applications requiring low-latency response or hardware platforms with limited resources, such as those served by edge computing environments. In this paper, we present a nomenclature for characterizing server-less function access patterns which allows us to derive the basic requirements of a serverless computing runtime. We then propose the use of {WebAssembly} as an alternative method for running serverless applications while meeting these requirements. Finally, we demonstrate how a {WebAssembly}-based serverless platform provides many of the same isolation and performance guarantees of container-based platforms while reducing average application start times and the resources needed to host them.},
	pages = {225--236},
	booktitle = {Proceedings of the International Conference on Internet of Things Design and Implementation},
	publisher = {Association for Computing Machinery},
	author = {Hall, Adam and Ramachandran, Umakishore},
	urldate = {2021-11-15},
	date = {2019-04-15},
	keywords = {{FaaS}, fog computing, function-as-a-service, serverless, edge computing, webassembly},
	file = {Full Text PDF:/home/volodia/Zotero/storage/UHCM86DR/Hall and Ramachandran - 2019 - An execution model for serverless functions at the.pdf:application/pdf;An execution model for serverless functions at the edge:/home/volodia/Zotero/storage/J7ISVEIS/hall2019.pdf.pdf:application/pdf},
}

@article{hellerstein_serverless_2018,
	title = {Serverless Computing: One Step Forward, Two Steps Back},
	url = {http://arxiv.org/abs/1812.03651},
	shorttitle = {Serverless Computing},
	abstract = {Serverless computing offers the potential to program the cloud in an autoscaling, pay-as-you go manner. In this paper we address critical gaps in first-generation serverless computing, which place its autoscaling potential at odds with dominant trends in modern computing: notably data-centric and distributed computing, but also open source and custom hardware. Put together, these gaps make current serverless offerings a bad fit for cloud innovation and particularly bad for data systems innovation. In addition to pinpointing some of the main shortfalls of current serverless architectures, we raise a set of challenges we believe must be met to unlock the radical potential that the cloud---with its exabytes of storage and millions of cores---should offer to innovative developers.},
	journaltitle = {{arXiv}:1812.03651 [cs]},
	author = {Hellerstein, Joseph M. and Faleiro, Jose and Gonzalez, Joseph E. and Schleier-Smith, Johann and Sreekanti, Vikram and Tumanov, Alexey and Wu, Chenggang},
	urldate = {2021-11-15},
	date = {2018-12-10},
	eprinttype = {arxiv},
	eprint = {1812.03651},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Databases},
	file = {arXiv.org Snapshot:/home/volodia/Zotero/storage/RBF7I69Z/1812.html:text/html;arXiv Fulltext PDF:/home/volodia/Zotero/storage/WH8KXSR6/Hellerstein et al. - 2018 - Serverless Computing One Step Forward, Two Steps .pdf:application/pdf},
}

@article{shillaker_provider-friendly_2018,
	title = {A Provider-Friendly Serverless Framework for Latency-Critical Applications},
	abstract = {Serverless is an important step in the evolution of cloud computing. First virtualisation enabled sharing a physical machine, then containers enabled sharing an operating system, now serverless targets sharing a runtime. Serverless platforms allow users to build distributed systems from individual functions, knowing little about the underlying infrastructure. They are free from concerns around configuration, maintenance and scalability. Meanwhile providers guarantee timely execution in a secure, isolated environment with pay-as-you-execute billing.},
	pages = {4},
	author = {Shillaker, Simon},
	date = {2018},
	langid = {english},
	file = {Shillaker - 2018 - A Provider-Friendly Serverless Framework for Laten.pdf:/home/volodia/Zotero/storage/82T9RYAV/Shillaker - 2018 - A Provider-Friendly Serverless Framework for Laten.pdf:application/pdf},
}

@inproceedings{rausch_towards_2019,
	title = {Towards a Serverless Platform for Edge \{{AI}\}},
	url = {https://www.usenix.org/conference/hotedge19/presentation/rausch},
	eventtitle = {2nd \{{USENIX}\} Workshop on Hot Topics in Edge Computing ({HotEdge} 19)},
	author = {Rausch, Thomas and Hummer, Waldemar and Muthusamy, Vinod and Rashed, Alexander and Dustdar, Schahram},
	urldate = {2021-11-15},
	date = {2019},
	langid = {english},
	file = {Snapshot:/home/volodia/Zotero/storage/ME97KH37/rausch.html:text/html;Full Text PDF:/home/volodia/Zotero/storage/7ZBDMWWM/Rausch et al. - 2019 - Towards a Serverless Platform for Edge \{AI\}.pdf:application/pdf},
}

@inproceedings{akkus_sand_2018,
	title = {\{{SAND}\}: Towards High-Performance Serverless Computing},
	isbn = {978-1-939133-01-4},
	url = {https://www.usenix.org/conference/atc18/presentation/akkus},
	shorttitle = {\{{SAND}\}},
	eventtitle = {2018 \{{USENIX}\} Annual Technical Conference (\{{USENIX}\} \{{ATC}\} 18)},
	pages = {923--935},
	author = {Akkus, Istemi Ekin and Chen, Ruichuan and Rimac, Ivica and Stein, Manuel and Satzke, Klaus and Beck, Andre and Aditya, Paarijaat and Hilt, Volker},
	urldate = {2021-11-15},
	date = {2018},
	langid = {english},
	file = {Full Text PDF:/home/volodia/Zotero/storage/NEVUPB5Y/Akkus et al. - 2018 - \{SAND\} Towards High-Performance Serverless Comput.pdf:application/pdf},
}

@online{noauthor_sand_nodate,
	title = {{SAND}: Towards High-Performance Serverless Computing {\textbar} {USENIX}},
	url = {https://www.usenix.org/conference/atc18/presentation/akkus},
	urldate = {2021-11-15},
	file = {SAND\: Towards High-Performance Serverless Computing | USENIX:/home/volodia/Zotero/storage/B3HPL4ZV/akkus.html:text/html},
}

@online{rocha_adlrocha_2020,
	title = {@adlrocha - Can {WASM} become the new Docker?},
	url = {https://adlrocha.substack.com/p/adlrocha-can-wasm-become-the-new},
	abstract = {{WASM} in the cloud with Krustlet},
	titleaddon = {@adlrocha Weekly Newsletter},
	type = {Substack newsletter},
	author = {Rocha, Alfonso de la},
	urldate = {2021-11-15},
	date = {2020-05-17},
	file = {Snapshot:/home/volodia/Zotero/storage/H8NVK435/adlrocha-can-wasm-become-the-new.html:text/html},
}

@online{brown_how_2021,
	title = {How {WASI} Makes Containerization More Efficient},
	url = {https://training.linuxfoundation.org/blog/how-wasi-makes-containerization-more-efficient/},
	abstract = {By Marco Fioretti {WebAssembly}, or Wasm for brevity, is a standardized binary format that allows software written in any language to run without customizations on any platform, inside sandboxes or...},
	titleaddon = {Linux Foundation - Training},
	author = {Brown, Dan},
	urldate = {2021-11-15},
	date = {2021-05-13},
	langid = {american},
	file = {Snapshot:/home/volodia/Zotero/storage/5JHE36E9/how-wasi-makes-containerization-more-efficient.html:text/html},
}

@software{noauthor_kubespherekubesphere_2021,
	title = {kubesphere/kubesphere},
	rights = {Apache-2.0},
	url = {https://github.com/kubesphere/kubesphere},
	abstract = {The container platform tailored for Kubernetes multi-cloud, datacenter, and edge management ⎈ 🖥 ☁️},
	publisher = {{KubeSphere}},
	urldate = {2021-11-24},
	date = {2021-11-24},
	note = {original-date: 2018-04-21T02:03:04Z},
	keywords = {cloud-native, cncf, container-management, devops, hacktoberfest, istio, jenkins, k8s, kubernetes, kubernetes-platform-solution, kubesphere, multi-cluster, observability, servicemesh},
}

@inproceedings{barcelona-pons_faas_2019,
	title = {On the {FaaS} Track: Building Stateful Distributed Applications with Serverless Architectures},
	isbn = {978-1-4503-7009-7},
	doi = {10.1145/3361525.3361535},
	shorttitle = {On the {FaaS} Track},
	abstract = {Serverless computing is an emerging paradigm that greatly simplifies the usage of cloud resources and suits well to many tasks. Most notably, Function-as-a-Service ({FaaS}) enables programmers to develop cloud applications as individual functions that can run and scale independently. Yet, due to the disaggregation of storage and compute resources in {FaaS}, applications that require fine-grained support for mutable state and synchronization, such as machine learning and scientific computing, are hard to build.
In this work, we present Crucial, a system to program highly-concurrent stateful applications with serverless architectures. Its programming model keeps the simplicity of {FaaS} and allows to port effortlessly multi-threaded algorithms to this new environment. Crucial is built upon the key insight that {FaaS} resembles to concurrent programming at the scale of a data center. As a consequence, a distributed shared memory layer is the right answer to the need for fine-grained state management and coordination in serverless. We validate our system with the help of micro-benchmarks and various applications. In particular, we implement two common machine learning algorithms: k-means clustering and logistic regression. For both cases, Crucial obtains superior or comparable performance to an equivalent Spark cluster.},
	pages = {41--54},
	author = {Barcelona-Pons, Daniel and Sánchez-Artigas, Marc and París, Gerard and Sutra, Pierre and López, Pedro},
	date = {2019-12-09},
}

@inproceedings{mvondo_ofc_2021,
	location = {Online Event United Kingdom},
	title = {{OFC}: an opportunistic caching system for {FaaS} platforms},
	isbn = {978-1-4503-8334-9},
	url = {https://dl.acm.org/doi/10.1145/3447786.3456239},
	doi = {10.1145/3447786.3456239},
	shorttitle = {{OFC}},
	abstract = {Cloud applications based on the “Functions as a Service” ({FaaS}) paradigm have become very popular. Yet, due to their stateless nature, they must frequently interact with an external data store, which limits their performance. To mitigate this issue, we introduce {OFC}, a transparent, vertically and horizontally elastic in-memory caching system for {FaaS} platforms, distributed over the worker nodes. {OFC} provides these benefits cost-effectively by exploiting two common sources of resource waste: (i) most cloud tenants overprovision the memory resources reserved for their functions because their footprint is non-trivially input-dependent and (ii) {FaaS} providers keep function sandboxes alive for several minutes to avoid cold starts. Using machine learning models adjusted for typical function input data categories (e.g., multimedia formats), {OFC} estimates the actual memory resources required by each function invocation and hoards the remaining capacity to feed the cache. We build our {OFC} prototype based on enhancements to the {OpenWhisk} {FaaS} platform, the Swift persistent object store, and the {RAMCloud} in-memory store. Using a diverse set of workloads, we show that {OFC} improves by up to 82 \% and 60 \% respectively the execution time of single-stage and pipelined functions.},
	eventtitle = {{EuroSys} '21: Sixteenth European Conference on Computer Systems},
	pages = {228--244},
	booktitle = {Proceedings of the Sixteenth European Conference on Computer Systems},
	publisher = {{ACM}},
	author = {Mvondo, Djob and Bacou, Mathieu and Nguetchouang, Kevin and Ngale, Lucien and Pouget, Stéphane and Kouam, Josiane and Lachaize, Renaud and Hwang, Jinho and Wood, Tim and Hagimont, Daniel and De Palma, Noël and Batchakui, Bernabé and Tchana, Alain},
	urldate = {2021-11-22},
	date = {2021-04-21},
	langid = {english},
	file = {Mvondo et al. - 2021 - OFC an opportunistic caching system for FaaS plat.pdf:/home/volodia/Zotero/storage/N4TA4XZ4/Mvondo et al. - 2021 - OFC an opportunistic caching system for FaaS plat.pdf:application/pdf},
}

@online{hykes_solomon_2019,
	title = {Solomon Hykes sur Twitter},
	url = {https://twitter.com/solomonstre/status/1111004913222324225},
	abstract = {If {WASM}+{WASI} existed in 2008, we wouldn't have needed to created Docker. That's how important it is. Webassembly on the server is the future of computing. A standardized system interface was the missing link. Let's hope {WASI} is up to the task!},
	titleaddon = {Twitter},
	author = {Hykes, Solomon},
	urldate = {2021-12-06},
	date = {2019},
	langid = {french},
}

@article{chaudhry_improved_2020,
	title = {Improved {QoS} at the Edge Using Serverless Computing to Deploy Virtual Network Functions},
	volume = {7},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2020.3011057},
	abstract = {Multiaccess edge computing ({MEC}) will strengthen forthcoming 5G networks by improving the Quality of Service ({QoS}), in particular, reducing latency, increasing data processing rates, and providing real-time information to develop high-value Internet-of-Things ({IoT}) services. To enable data-intensive network services and support advanced analytics, many network operators have proposed to integrate {MEC} systems with network function virtualization ({NFV}) consolidating virtual network functions ({VNFs}) and edge capabilities on a shared infrastructure. As of yet, this integration is not fully established, with various architectural issues currently open, even at standardization level. For instance, any update to {VNFs} deployed in a {MEC} system requires a time-consuming manual effort, which affects the overall infrastructure operations. To address these pitfalls, {VNFs} can be decomposed into microservices, which maintain their own states and exhibit different resource consumption requirements. This article presents an approach to integration that leverages serverless computing to merge {MEC} and {NFV} at the system level and to deploy {VNFs} on demand, by combining {MEC} functional blocks with an {NFV} orchestrator using a Kubernetes cluster. We further investigate whether the resource utilization of a {MEC} system can be improved by leveraging networked {FPGA}-enabled {MEC} servers, through an extension of the edge layer that takes advantage of available programmable hardware. We quantitatively evaluate and demonstrate the improvement of 75\% end-to-end latency, 99.96\% {VNF} execution time, 26.9\% resource utilization, and 15.8\% energy consumption in comparison with traditional baselines of cloud, edge, and serverless-edge test cases for a high-definition real-time video streaming application.},
	pages = {10673--10683},
	number = {10},
	journaltitle = {{IEEE} Internet of Things Journal},
	author = {Chaudhry, Saqib Rasool and Palade, Andrei and Kazmi, Aqeel and Clarke, Siobhán},
	date = {2020-10},
	note = {Conference Name: {IEEE} Internet of Things Journal},
	keywords = {Edge computing, serverless computing, Internet of Things, Task analysis, Servers, Quality of service, Streaming media, Field programmable gate arrays, Hardware, networked {FPGA}, virtual network functions ({VNFs}) and data security},
}

@article{shi_when_2021,
	title = {When Blockchain Meets Auction Models: A Survey, Some Applications, and Challenges},
	url = {http://arxiv.org/abs/2110.12534},
	shorttitle = {When Blockchain Meets Auction Models},
	abstract = {In recent years, blockchain has gained widespread attention as an emerging technology for decentralization, transparency, and immutability in advancing online activities over public networks. As an essential market process, auctions have been well studied and applied in many business fields due to their efficiency and contributions to fair trade. Complementary features between blockchain and auction models trigger a great potential for research and innovation. On the one hand, the decentralized nature of blockchain can provide a trustworthy, secure, and cost-effective mechanism to manage the auction process; on the other hand, auction models can be utilized to design incentive and consensus protocols in blockchain architectures. These opportunities have attracted enormous research and innovation activities in both academia and industry; however, there is a lack of an in-depth review of existing solutions and achievements. In this paper, we conduct a comprehensive state-of-the-art survey of these two research topics. We review the existing solutions for integrating blockchain and auction models, with some application-oriented taxonomies generated. Additionally, we highlight some open research challenges and future directions towards integrated blockchain-auction models.},
	journaltitle = {{arXiv}:2110.12534 [cs]},
	author = {Shi, Zeshun and de Laat, Cees and Grosso, Paola and Zhao, Zhiming},
	urldate = {2021-12-06},
	date = {2021-10-24},
	eprinttype = {arxiv},
	eprint = {2110.12534},
	keywords = {Computer Science - Networking and Internet Architecture, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/home/volodia/Zotero/storage/96L7BSSF/Shi et al. - 2021 - When Blockchain Meets Auction Models A Survey, So.pdf:application/pdf},
}

@article{debe_blockchain-based_2020,
	title = {Blockchain-Based Decentralized Reverse Bidding in Fog Computing},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2991261},
	abstract = {Fog computing systems are designed to provide localized computation, storage, and communication services in close proximity to the endpoint mobile and {IoT} devices. Fog service providers typically monetize their service usage via centralized payment mechanisms in unverifiable and non-transparent manner. Therefore, there exists a need for a trust-enabling payment mechanism whereby fog service providers should be incentivized or penalized based upon the continuous feedback from endpoint devices. We propose a decentralized reverse-bidding scheme developed using the key features of blockchain and smart contracts. We develop a solution that allows the users or devices to initiate the bidding process by making a request for services to be provided by nearby public fog nodes, and these fog nodes to make bid offers in return. The proposed scheme ensures that all fog nodes on the network can equally and fairly make offers to win the bid. The bidding process incorporates the automated payments at the end of the service. Our solution is implemented using Ethereum smart contracts. It also integrates a reputation system for fog nodes and imposes a penalty for misbehaving nodes. Our solution is fully decentralized and provides a high level of trust, transparency, and security. In the paper, we present the system architecture, implementation details, and show the correct functionality of the overall proposed solution. In addition, we provide performance, cost, and security analyses of the smart contract code to demonstrate its effectiveness and robustness against major security concerns. The results show that the cost of running the smart contract remained less than three cents with the current Ethereum price (i.e., 183.22 {USD}/Eth). We have also made our smart contract code publicly available on Github.},
	pages = {81686--81697},
	journaltitle = {{IEEE} Access},
	author = {Debe, Mazin and Salah, Khaled and Rehman, Muhammad Habib Ur and Svetinovic, Davor},
	date = {2020},
	note = {Conference Name: {IEEE} Access},
	keywords = {Cloud computing, Edge computing, {IoT}, fog computing, Servers, Blockchain, Quality of service, auctioning, bidding, Ethereum, Security, smart contracts, Smart contracts},
	file = {IEEE Xplore Full Text PDF:/home/volodia/Zotero/storage/D7RLMX3W/Debe et al. - 2020 - Blockchain-Based Decentralized Reverse Bidding in .pdf:application/pdf},
}

@inproceedings{franco_brain_2019,
	title = {{BRAIN}: Blockchain-based Reverse Auction for Infrastructure Supply in Virtual Network Functions-as-a -Service},
	doi = {10.23919/IFIPNetworking.2019.8816843},
	shorttitle = {{BRAIN}},
	abstract = {Network Functions Virtualization ({NFV}) is transforming the way in which network operators acquire and manage network services. By using virtualization technologies to move packet processing from dedicated hardware to software, {NFV} has introduced a new market focused on the offer and distribution of Virtual Network Functions ({VNF}). Infrastructure Providers ({InP}) can benefit from an {NFV} market by providing their infrastructures to fulfill demands of end-users that, in turn, acquire {VNFs}-as-a-Service ({VNFaaS}). In this context, solutions that promote the competition between {InPs} can lead to lower prices, while increasing {VNF} performance to accommodate specific demands of end-users. In this paper, {BRAIN}, a blockchain-based reverse auction is presented to introduce an auditable solution in which {InPs} can compete to host {VNFs} taking into account the demands of each particular end-user. Such a solution helps reduce costs involved in {VNF}'s commercialization and also monetize {NFV}-enabled infrastructures. {BRAIN} is supported by a case study that provides evidence of the solution's feasibility and effectiveness. A discussion regarding blockchain advantages and drawbacks in this use-case (e.g., additional costs and time) concludes this paper.},
	eventtitle = {2019 {IFIP} Networking Conference ({IFIP} Networking)},
	pages = {1--9},
	booktitle = {2019 {IFIP} Networking Conference ({IFIP} Networking)},
	author = {Franco, Muriel Figueredo and Scheid, Eder John and Granville, Lisandro Zambenedetti and Stiller, Burkhard},
	date = {2019-05},
	note = {{ISSN}: 1861-2288},
	keywords = {Blockchain, Infrastructure Supply, Network Functions Virtualization, Smart Contract, Virtual Net-work Functions-as-a-Service},
	file = {Accepted Version:/home/volodia/Zotero/storage/PGYSHZXX/Franco et al. - 2019 - BRAIN Blockchain-based Reverse Auction for Infras.pdf:application/pdf},
}

@incollection{yu_building_2019,
	title = {Building Trustful Crowdsensing Service on the Edge},
	isbn = {978-3-030-23596-3},
	abstract = {Edge computing enables the data to be processed in the edge of networks in order to decrease the latency of crowdsensing services. However, due to the distributed environment and vulnerability of edges, it is difficult for different edges to reach consistency to provide the same service and protect the data from tampering at the same time. To solve these problems, the Blockchain, a credible and natural decentralized technique, is considered as a suitable tool. In this paper, we proposed a Blockchain-based edge crowdsensing service system in which the edge runs a changeable auction algorithm for every task that the users request to find a winner who can provide corresponding sensing data. Specifically, based on {PBFT} algorithm, we proposed a consensus algorithm named Leader Stable Practical Byzantine Fault Tolerance ({LS}-{PBFT}). This algorithm enables all edges to collaboratively maintain an updated, consistent and credible ledger in Blockchain. Furthermore, the data generated in this process are constructed as a multi-transaction, which can be packaged into a block and stored in the block. Simulation results reveal that the proposed system is not only efficient in generating and storing blocks but also feasible in resisting attacks of malicious users and edges. Our experiments also show that {LS}-{PBFT} takes less than 50\% of the time cost by {PBFT} to reach consensus.},
	pages = {445--457},
	author = {Yu, Biao and Chen, Yingwen and Fu, Shaojing and Yu, Wanrong and Guo, Xiaoli},
	date = {2019-06-21},
	doi = {10.1007/978-3-030-23597-0_36},
}

@article{du_collaborative_nodate,
	title = {Collaborative crowdsensing at the edge},
	pages = {154},
	author = {Du, Yifan},
	langid = {english},
	file = {Du - Collaborative crowdsensing at the edge.pdf:/home/volodia/Zotero/storage/VAKX98DU/Du - Collaborative crowdsensing at the edge.pdf:application/pdf},
}

@inproceedings{zavodovski_decloud_2019,
	title = {{DeCloud}: Truthful Decentralized Double Auction for Edge Clouds},
	doi = {10.1109/ICDCS.2019.00212},
	shorttitle = {{DeCloud}},
	abstract = {The sharing economy has made great inroads with services like Uber or Airbnb enabling people to share their unused resources with those needing them. The computing world, however, despite its abundance of excess computational resources has remained largely unaffected by this trend, save for few examples like {SETI}@home. We present {DeCloud}, a decentralized market framework bringing the sharing economy to on-demand computing where the offering of pay-as-you-go services will not be limited to large companies, but ad hoc clouds can be spontaneously formed on the edge of the network. We design incentive compatible double auction mechanism targeted specifically for distributed ledger trust model instead of relying on third-party auctioneer. {DeCloud} incorporates innovative matching heuristic capable of coping with the level of heterogeneity inherent for large-scale open systems. Evaluating {DeCloud} on Google cluster-usage data, we demonstrate that the system has a near-optimal performance from an economic point of view, additionally enhanced by the flexibility of matching.},
	eventtitle = {2019 {IEEE} 39th International Conference on Distributed Computing Systems ({ICDCS})},
	pages = {2157--2167},
	booktitle = {2019 {IEEE} 39th International Conference on Distributed Computing Systems ({ICDCS})},
	author = {Zavodovski, Aleksandr and Bayhan, Suzan and Mohan, Nitinder and Zhou, Pengyuan and Wong, Walter and Kangasharju, Jussi},
	date = {2019-07},
	note = {{ISSN}: 2575-8411},
	keywords = {Cloud computing, Cloud Computing, Edge computing, Edge Computing, Economics, Blockchain, Smart contracts, Crowdsourcing, Distributed ledger, Incentive Compatible Auction, Mechanism design, Smart Contracts, Truthful Auction},
}

@online{noauthor_cloud_nodate,
	title = {Cloud {IoT} Core},
	url = {https://cloud.google.com/iot-core},
	abstract = {Easily and securely connect, manage, and ingest data from globally dispersed devices.},
	titleaddon = {Google Cloud},
	urldate = {2021-12-06},
	langid = {english},
}

@online{noauthor_iot_nodate,
	title = {{IoT} Edge {\textbar} Cloud Intelligence {\textbar} Microsoft Azure},
	url = {https://azure.microsoft.com/en-us/services/iot-edge/},
	abstract = {Connect cloud intelligence to your edge devices with Azure {IoT} Edge, a comprehensive service that deploys artificial intelligence and custom logic to {IoT} devices.},
	urldate = {2021-12-06},
	langid = {english},
}

@online{noauthor_aws_nodate,
	title = {{AWS} {IoT} Greengrass - Amazon Web Services},
	url = {https://aws.amazon.com/fr/greengrass/},
	titleaddon = {Amazon Web Services, Inc.},
	urldate = {2021-12-06},
	langid = {french},
}

@online{noauthor_apache_nodate,
	title = {Apache {OpenWhisk} is a serverless, open source cloud platform},
	url = {https://openwhisk.apache.org/},
	urldate = {2021-12-06},
}

@online{noauthor_esp8266_nodate,
	title = {{ESP}8266 Wi-Fi {MCU} I Espressif Systems},
	url = {https://www.espressif.com/en/products/socs/esp8266},
	urldate = {2021-12-06},
}

@article{lage-freitas_cloud_2017,
	title = {Cloud resource management driven by profit augmentation},
	volume = {29},
	issn = {1532-0634},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.3899},
	doi = {10.1002/cpe.3899},
	abstract = {Cloud computing has become the infrastructure of choice for building and delivering software services. A key challenge for service providers is effectively managing cloud resources in order to increase profit while maintaining service-level agreements ({SLAs}) with customers. To address this challenge, this paper proposes a combination of automated mechanisms for resource and execution management. The resource management mechanisms, namely, under-provisioning and contract rescission, reduce resource allocation costs and minimize penalties incurred when performance objectives are violated. The execution management mechanisms, namely, crash recovery and delay recovery, minimize penalties incurred when reliability objectives are violated. The mechanisms are integrated in the Qu4DS framework and evaluated in the Grid'5000 testbed. The results show that the under-provisioning mechanism increases profit by 20–50\%, the contract rescission mechanism increases profit up to four times, and the execution management mechanisms increase profit by up to 60\%. Copyright © 2016 John Wiley \& Sons, Ltd.},
	pages = {e3899},
	number = {4},
	journaltitle = {Concurrency and Computation: Practice and Experience},
	author = {Lage-Freitas, André and Parlavantzas, Nikos and Pazat, Jean},
	urldate = {2021-12-07},
	date = {2017},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.3899},
	keywords = {resource management, profit maximization, service-level objectives ({SLOs}), {SLA} enforcement},
}

@software{noauthor_firecracker-microvmfirecracker_2021-1,
	title = {firecracker-microvm/firecracker},
	rights = {Apache-2.0},
	url = {https://github.com/firecracker-microvm/firecracker},
	abstract = {Secure and fast {microVMs} for serverless computing.},
	publisher = {firecracker-microvm},
	urldate = {2021-12-10},
	date = {2021-12-10},
	note = {original-date: 2017-10-19T06:18:47Z},
	keywords = {serverless, containers, minimalist, open-source, oversubscription, rust, sandbox, virtual-machine, virtualization},
}

@article{ieee_standards_association_ieee_2018,
	title = {{IEEE} Standard for Adoption of {OpenFog} Reference Architecture for Fog Computing},
	doi = {10.1109/IEEESTD.2018.8423800},
	abstract = {{OpenFog} Consortium–{OpenFog} Reference Architecture for Fog Computing is adopted by this standard. {OpenFog} Reference Architecture [{OPFRA}001.020817] is a structural and functional prescription of an open, interoperable, horizontal system architecture for distributing computing, storage, control and networking functions closer to the users along a cloud-to-thing continuum of communicating, computing, sensing and actuating entities. It encompasses various approaches to disperse Information Technology ({IT}), Communication Technology ({CT}) and Operational Technology ({OT}) Services through information messaging infrastructure as well as legacy and emerging multi-access networking technologies.},
	pages = {1--176},
	journaltitle = {{IEEE} Std 1934-2018},
	author = {{IEEE Standards Association}},
	date = {2018-08},
	note = {Conference Name: {IEEE} Std 1934-2018},
	keywords = {Edge computing, adoption, communication technology {IEEE} 1934™, Communications technology, {IEEE} Standards, information technology, Information technology, {OpenFog}, operational technology},
	file = {IEEE Standard for Adoption of OpenFog Reference Architecture for Fog Computing:/home/volodia/Zotero/storage/8W8P3SSH/ieee-standard-for-adoption-of-openfog-reference-architecture-for.pdf.pdf:application/pdf},
}

@inreference{wikipedia_edge_2021,
	title = {Edge computing},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Edge_computing&oldid=1057908860},
	abstract = {Edge computing is a distributed computing paradigm that brings computation and data storage closer to the sources of data. This is expected to improve response times and save bandwidth. "A common misconception is that edge and {IoT} are synonymous. Edge computing is a topology- and location-sensitive form of distributed computing, while {IoT} is a use case instantiation of edge computing." The term refers to an architecture rather than a specific technology.The origins of edge computing lie in content distributed network that were created in the late 1990s to serve web and video content from edge servers that were deployed close to users. In the early 2000s, these networks evolved to host applications and application components at the edge servers, resulting in the first commercial edge computing services that hosted applications such as dealer locators, shopping carts, real-time data aggregators, and ad insertion engines.},
	booktitle = {Wikipedia},
	author = {{Wikipedia}},
	urldate = {2021-12-12},
	date = {2021-11-30},
	langid = {english},
	note = {Page Version {ID}: 1057908860},
}

@inreference{wikipedia_service-level_2021,
	title = {Service-level agreement},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Service-level_agreement&oldid=1045280242},
	abstract = {A service-level agreement ({SLA}) is a commitment between a service provider and a client. Particular aspects of the service – quality, availability, responsibilities – are agreed between the service provider and the service user. The most common component of an {SLA} is that the services should be provided to the customer as agreed upon in the contract. As an example, Internet service providers and telcos will commonly include service level agreements within the terms of their contracts with customers to define the level(s) of service being sold in plain language terms. In this case the {SLA} will typically have a technical definition in  mean time between failures ({MTBF}), mean time to repair or mean time to recovery ({MTTR}); identifying which party is responsible for reporting faults or paying fees; responsibility for various data rates; throughput; jitter; or similar measurable details.},
	booktitle = {Wikipedia},
	author = {{Wikipedia}},
	urldate = {2021-12-13},
	date = {2021-09-19},
	langid = {english},
	note = {Page Version {ID}: 1045280242},
}

@article{hang_sla-based_2019,
	title = {{SLA}-Based Sharing Economy Service with Smart Contract for Resource Integrity in the Internet of Things},
	volume = {9},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2076-3417/9/17/3602},
	doi = {10.3390/app9173602},
	abstract = {Recently, technology startups have leveraged the potential of blockchain-based technologies to govern institutions or interpersonal trust by enforcing signed treaties among different individuals in a decentralized environment. However, it is going to be hard enough convincing that the blockchain technology could completely replace the trust among trading partners in the sharing economy as sharing services always operate in a highly dynamic environment. With the rapid expanding of the rental market, the sharing economy faces more and more severe challenges in the form of regulatory uncertainty and concerns about abuses. This paper proposes an enhanced decentralized sharing economy service using the service level agreement ({SLA}), which documents the services the provider will furnish and defines the service standards the provider is obligated to meet. The {SLA} specifications are defined as the smart contract, which facilitates multi-user collaboration and automates the process with no involvement of the third party. To demonstrate the usability of the proposed solution in the sharing economy, a notebook sharing case study is implemented using the Hyperledger Fabric. The functionalities of the smart contract are tested using the Hyperledger Composer. Moreover, the efficiency of the designed approach is demonstrated through a series of experimental tests using different performance metrics.},
	pages = {3602},
	number = {17},
	journaltitle = {Applied Sciences},
	author = {Hang, Lei and Kim, Do-Hyeun},
	urldate = {2021-12-13},
	date = {2019-01},
	langid = {english},
	note = {Number: 17
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {blockchain, Hyperledger Fabric, service level agreement, sharing economy, smart contract},
	file = {SLA-Based Sharing Economy Service with Smart Contract for Resource Integrity in the Internet of Things:/home/volodia/Zotero/storage/I6Q4LQTJ/hang2019.pdf.pdf:application/pdf;Full Text PDF:/home/volodia/Zotero/storage/28D5UCB7/Hang and Kim - 2019 - SLA-Based Sharing Economy Service with Smart Contr.pdf:application/pdf},
}

@article{di_pascale_smart_2017,
	title = {Smart Contract {SLAs} for Dense Small-Cell-as-a-Service},
	url = {http://arxiv.org/abs/1703.04502},
	abstract = {The disruptive power of blockchain technologies represents a great opportunity to re-imagine standard practices of telecommunication networks and to identify critical areas that can benefit from brand new approaches. As a starting point for this debate, we look at the current limits of infrastructure sharing, and specifically at the Small-Cell-as-a-Service trend, asking ourselves how we could push it to its natural extreme: a scenario in which any individual home or business user can become a service provider for mobile network operators, freed from all the scalability and legal constraints that are inherent to the current modus operandi. We propose the adoption of smart contracts to implement simple but effective Service Level Agreements ({SLAs}) between small cell providers and mobile operators, and present an example contract template based on the Ethereum blockchain.},
	journaltitle = {{arXiv}:1703.04502 [cs]},
	author = {Di Pascale, Emanuele and {McMenamy}, Jasmina and Macaluso, Irene and Doyle, Linda},
	urldate = {2021-12-13},
	date = {2017-03-13},
	eprinttype = {arxiv},
	eprint = {1703.04502},
	keywords = {Computer Science - Networking and Internet Architecture},
	file = {arXiv Fulltext PDF:/home/volodia/Zotero/storage/VXR3GYJ8/Di Pascale et al. - 2017 - Smart Contract SLAs for Dense Small-Cell-as-a-Serv.pdf:application/pdf},
}

@inproceedings{zhou_trustworthy_2018,
	title = {Trustworthy Cloud Service Level Agreement Enforcement with Blockchain Based Smart Contract},
	doi = {10.1109/CloudCom2018.2018.00057},
	abstract = {Cloud Service Level Agreement ({SLA}) is challengeable due to lacking a trustworthy platform. This paper presents a witness model to credibly enforce the cloud service level agreement. Through introducing the witness role and using the blockchain based smart contract, we solve the trust issues about who can detect the service violation, how the violation is confirmed and the compensation is guaranteed. In this model, a verifiable consensus sortition algorithm proposed by us is firstly leveraged to select independent witnesses to form a witness committee. They are responsible for a specific service level agreement and get paid by monitoring and detecting service violation. Through carefully designing the witness' payoff function in the agreement, we further leverage game theory to analyze and prove that it is not the witness itself is trustworthy. Instead, the witness has to tell the truth because of its greedy nature, which is the desire to maximize its own revenue. As long as the service violation is confirmed by the witness committee, the compensation is automatically transferred to the customer by the smart contract. Finally, we implement a proof-of-concept prototype with the smart contract of Ethereum blockchain. It demonstrates the feasibility of our model.},
	eventtitle = {2018 {IEEE} International Conference on Cloud Computing Technology and Science ({CloudCom})},
	pages = {255--260},
	booktitle = {2018 {IEEE} International Conference on Cloud Computing Technology and Science ({CloudCom})},
	author = {Zhou, Huan and de Laat, Cees and Zhao, Zhiming},
	date = {2018-12},
	note = {{ISSN}: 2330-2186},
	keywords = {Cloud computing, Quality of service, Game theory, Monitoring, service level agreement, cloud computing, smart contract, blockchain, Systems architecture},
	file = {Trustworthy Cloud Service Level Agreement Enforcement with Blockchain Based Smart Contract:/home/volodia/Zotero/storage/AFST4S2F/10.1109@CloudCom2018.2018.00057.pdf.pdf:application/pdf;Submitted Version:/home/volodia/Zotero/storage/93R9JKIP/Zhou et al. - 2018 - Trustworthy Cloud Service Level Agreement Enforcem.pdf:application/pdf},
}

@inproceedings{maurice_hello_2017,
	location = {San Diego, {CA}},
	title = {Hello from the Other Side: {SSH} over Robust Cache Covert Channels in the Cloud},
	isbn = {978-1-891562-46-4},
	url = {https://www.ndss-symposium.org/ndss2017/ndss-2017-programme/hello-other-side-ssh-over-robust-cache-covert-channels-cloud/},
	doi = {10.14722/ndss.2017.23294},
	shorttitle = {Hello from the Other Side},
	abstract = {Covert channels evade isolation mechanisms between multiple parties in the cloud. Especially cache covert channels allow the transmission of several hundred kilobits per second between unprivileged user programs in separate virtual machines. However, caches are small and shared and thus cache-based communication is susceptible to noise from any system activity and interrupts. The feasibility of a reliable cache covert channel under a severe noise scenario has not been demonstrated yet. Instead, previous work relies on either of the two contradicting assumptions: the assumption of direct applicability of error-correcting codes, or the assumption that noise effectively prevents covert channels.},
	eventtitle = {Network and Distributed System Security Symposium},
	booktitle = {Proceedings 2017 Network and Distributed System Security Symposium},
	publisher = {Internet Society},
	author = {Maurice, Clementine and Weber, Manuel and Schwarz, Michael and Giner, Lukas and Gruss, Daniel and Boano, Carlo Alberto and Mangard, Stefan and Roemer, Kay and Mangard, Stefan},
	urldate = {2021-12-13},
	date = {2017},
	langid = {english},
	file = {Maurice et al. - 2017 - Hello from the Other Side SSH over Robust Cache C.pdf:/home/volodia/Zotero/storage/FMGKD9W6/Maurice et al. - 2017 - Hello from the Other Side SSH over Robust Cache C.pdf:application/pdf},
}

@article{bocci_secure_2021,
	title = {Secure {FaaS} orchestration in the fog: how far are we?},
	volume = {103},
	issn = {1436-5057},
	url = {https://doi.org/10.1007/s00607-021-00924-y},
	doi = {10.1007/s00607-021-00924-y},
	shorttitle = {Secure {FaaS} orchestration in the fog},
	abstract = {Function-as-a-Service ({FaaS}) allows developers to define, orchestrate and run modular event-based pieces of code on virtualised resources, without the burden of managing the underlying infrastructure nor the life-cycle of such pieces of code. Indeed, {FaaS} providers offer resource auto-provisioning, auto-scaling and pay-per-use billing at no costs for idle time. This makes it easy to scale running code and it represents an effective and increasingly adopted way to deliver software. This article aims at offering an overview of the existing literature in the field of next-gen {FaaS} from three different perspectives: (i) the definition of {FaaS} orchestrations, (ii) the execution of {FaaS} orchestrations in Fog computing environments, and (iii) the security of {FaaS} orchestrations. Our analysis identify trends and gaps in the literature, paving the way to further research on securing {FaaS} orchestrations in Fog computing landscapes.},
	pages = {1025--1056},
	number = {5},
	journaltitle = {Computing},
	shortjournal = {Computing},
	author = {Bocci, Alessandro and Forti, Stefano and Ferrari, Gian-Luigi and Brogi, Antonio},
	urldate = {2021-12-13},
	date = {2021-05-01},
	langid = {english},
	file = {Secure FaaS orchestration in the fog\: how far are we?:/home/volodia/Zotero/storage/DWWWJ6C3/bocci2021.pdf.pdf:application/pdf;Springer Full Text PDF:/home/volodia/Zotero/storage/I4ADMKNT/Bocci et al. - 2021 - Secure FaaS orchestration in the fog how far are .pdf:application/pdf},
}

@inproceedings{bermbach_towards_2020,
	title = {Towards Auction-Based Function Placement in Serverless Fog Platforms},
	doi = {10.1109/ICFC49376.2020.00012},
	abstract = {The Function-as-a-Service ({FaaS}) paradigm has a lot of potential as a computing model for fog environments comprising both cloud and edge nodes. When the request rate exceeds capacity limits at the edge, some functions need to be offloaded from the edge towards the cloud.In this position paper, we propose an auction-based approach in which application developers bid on resources. This allows fog nodes to make a local decision about which functions to offload while maximizing revenue. For a first evaluation of our approach, we use simulation.},
	eventtitle = {2020 {IEEE} International Conference on Fog Computing ({ICFC})},
	pages = {25--31},
	booktitle = {2020 {IEEE} International Conference on Fog Computing ({ICFC})},
	author = {Bermbach, David and Maghsudi, Setareh and Hasenburg, Jonathan and Pfandzelter, Tobias},
	date = {2020-04},
	keywords = {Cloud computing, Edge computing, Fog Computing, {FAA}, Resource management, Computational modeling, Bandwidth, Function-as-a-Service, Serverless Computing, Tools},
	file = {Towards Auction-Based Function Placement in Serverless Fog Platforms:/home/volodia/Zotero/storage/M2SY8GNY/bermbach2020.pdf.pdf:application/pdf;Submitted Version:/home/volodia/Zotero/storage/NZZQQEY8/Bermbach et al. - 2020 - Towards Auction-Based Function Placement in Server.pdf:application/pdf},
}

@inproceedings{baresi_towards_2019,
	title = {Towards a Serverless Platform for Edge Computing},
	doi = {10.1109/ICFC.2019.00008},
	abstract = {The emergence of real-time and data-intensive applications empowered by mobile computing and {IoT} devices is challenging the success of centralized data centers, and fostering the adoption of the paradigm of fog/edge computing. Differently from cloud data centers, fog nodes are geographically distributed in proximity to data prosumers, taking advantage of the emerging wireless communication technologies and mobile networks. The limited resources of densely distributed fog nodes call for their efficient use by hosted applications and services. To address this challenge, and the needs of different application scenarios, this paper proposes a serverless platform for edge computing. It starts motivating the adoption of a serverless architecture. Then, it presents the services and mechanisms that are the building blocks of a Serverless Edge Platform. The paper also proposes a prototype platform and its assessment. Obtained results demonstrate the feasibility of the proposed solution for satisfying different application requirements in diverse deployment configurations of heterogeneous fog nodes.},
	eventtitle = {2019 {IEEE} International Conference on Fog Computing ({ICFC})},
	pages = {1--10},
	booktitle = {2019 {IEEE} International Conference on Fog Computing ({ICFC})},
	author = {Baresi, Luciano and Filgueira Mendonça, Danilo},
	date = {2019-06},
	keywords = {Cloud computing, Computer architecture, Edge computing, {FAA}, Computational modeling, Data centers, Containers, data-intensive, edge-computing, fog-computing, latency-sensitive, serverless-computing},
	file = {Towards a Serverless Platform for Edge Computing:/home/volodia/Zotero/storage/4HAWYLYL/10.1109@ICFC.2019.00008.pdf.pdf:application/pdf},
}

@inproceedings{baresi_paps_2019,
	location = {Cham},
	title = {{PAPS}: A Framework for Decentralized Self-management at the Edge},
	isbn = {978-3-030-33702-5},
	doi = {10.1007/978-3-030-33702-5_39},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{PAPS}},
	abstract = {The emergence of latency-sensitive and data-intensive applications requires that computational resources be moved closer to users on computing nodes at the edge of the network (edge computing). Since these nodes have limited resources, the collaboration among them is critical for the robustness, performance, and scalability of the system. One must allocate and provision computational resources to the different components, and these components must be placed on the nodes by considering both network latency and resource availability. Since centralized solutions could be impracticable for large-scale systems, this paper presents {PAPS} (Partitioning, Allocation, Placement, and Scaling), a framework that tackles the complexity of edge infrastructures by means of decentralized self-management and serverless computing. First, the large-scale edge topology is dynamically partitioned into delay-aware communities. Community leaders then provide a reference allocation of resources and tackle the intricate placement of the containers that host serverless functions. Finally, control theory is used at the node level to scale resources timely and effectively. The assessment shows both the feasibility of the approach and its ability to tackle the placement and allocation problem for large-scale edge topologies with up to 100 serverless functions and intense and unpredictable workload variations.},
	pages = {508--522},
	booktitle = {Service-Oriented Computing},
	publisher = {Springer International Publishing},
	author = {Baresi, Luciano and Mendonça, Danilo Filgueira and Quattrocchi, Giovanni},
	editor = {Yangui, Sami and Bouassida Rodriguez, Ismael and Drira, Khalil and Tari, Zahir},
	date = {2019},
	langid = {english},
	keywords = {Edge computing, Resource management, Serverless computing, Geo-distributed infrastructures, Service placement},
}

@inproceedings{baresi_empowering_2017,
	title = {Empowering Low-Latency Applications Through a Serverless Edge Computing Architecture},
	volume = {{LNCS}-10465},
	url = {https://hal.inria.fr/hal-01677622},
	doi = {10.1007/978-3-319-67262-5_15},
	abstract = {The exponential increase of the data generated by pervasive and mobile devices requires disrupting approaches for the realization of emerging mobile and {IoT} applications. Although cloud computing provides virtually unlimited computational resources, low-latency applications cannot afford the high latencies introduced by sending and retrieving data from/to the cloud. In this scenario, edge computing appears as a promising solution by bringing computation and data near to users and devices. However, the resource-finite nature of edge servers constrains the possibility of deploying full applications on them. To cope with these problems, we propose a serverless architecture at the edge, bringing a highly scalable, intelligent and cost-effective use of edge infrastructure’s resources with minimal configuration and operation efforts. The feasibility of our approach is shown through an augmented reality use case for mobile devices, in which we offload computation and data intensive tasks from the devices to serverless functions at the edge, outperforming the cloud alternative up to 80\% in terms of throughput and latency.},
	eventtitle = {6th European Conference on Service-Oriented and Cloud Computing ({ESOCC})},
	pages = {196},
	publisher = {Springer International Publishing},
	author = {Baresi, Luciano and Mendonça, Danilo Filgueira and Garriga, Martin},
	urldate = {2021-12-14},
	date = {2017-09-27},
	langid = {english},
	file = {Empowering Low-Latency Applications Through a Serverless Edge Computing Architecture:/home/volodia/Zotero/storage/X69IKTIF/baresi2017.pdf.pdf:application/pdf;Full Text PDF:/home/volodia/Zotero/storage/CCSGDR89/Baresi et al. - 2017 - Empowering Low-Latency Applications Through a Serv.pdf:application/pdf},
}

@incollection{baresi_paps_2019-1,
	title = {{PAPS}: A Framework for Decentralized Self-management at the Edge},
	isbn = {978-3-030-33701-8},
	shorttitle = {{PAPS}},
	abstract = {The emergence of latency-sensitive and data-intensive applications requires that computational resources be moved closer to users on computing nodes at the edge of the network (edge computing). Since these nodes have limited resources, the collaboration among them is critical for the robustness, performance, and scalability of the system. One must allocate and provision computational resources to the different components, and these components must be placed on the nodes by considering both network latency and resource availability. Since centralized solutions could be impracticable for large-scale systems, this paper presents {PAPS} (Partitioning, Allocation, Placement, and Scaling), a framework that tackles the complexity of edge infrastructures by means of decentralized self-management and serverless computing. First, the large-scale edge topology is dynamically partitioned into delay-aware communities. Community leaders then provide a reference allocation of resources and tackle the intricate placement of the containers that host serverless functions. Finally, control theory is used at the node level to scale resources timely and effectively. The assessment shows both the feasibility of the approach and its ability to tackle the placement and allocation problem for large-scale edge topologies with up to 100 serverless functions and intense and unpredictable workload variations.},
	pages = {508--522},
	author = {Baresi, Luciano and Mendonça, Danilo and Quattrocchi, Giovanni},
	date = {2019-10-22},
	doi = {10.1007/978-3-030-33702-5_39},
	file = {Full Text PDF:/home/volodia/Zotero/storage/4QP9F6YF/Baresi et al. - 2019 - PAPS A Framework for Decentralized Self-managemen.pdf:application/pdf},
}

@article{misra_fogprime_2021,
	title = {{FogPrime}: Dynamic Pricing-Based Strategic Resource Management in Fog Networks},
	volume = {70},
	issn = {1939-9359},
	doi = {10.1109/TVT.2021.3096149},
	shorttitle = {{FogPrime}},
	abstract = {In this paper, the problem of strategic resource management in fog networks is discussed while considering a pay-per-use model, similar to that used in cloud. Fog networks are distributed in nature, because of which resource management in these networks is an {NP}-hard problem. In the existing literature, the researchers focused on resource management in fog networks, while considering the network delay constraint. However, none of these works considered the effect of pricing policy while deciding on resource allocation. Hence, there is a need for pricing-based resource management in fog networks. In this work, we proposed a dynamic pricing-based resource allocation scheme, named {FogPrime}, for analyzing the trade-off between the service delay and the associated price. In {FogPrime}, we use dynamic coalition-formation game to decide the resource allocation strategy locally within a cluster. On the other hand, we use utility game to choose the fog nodes, strategically, while considering the aforementioned trade-off. Through simulation, we observed that {FogPrime} outperforms the existing schemes in terms of satisfaction of the involved entities - the end-user and the fog nodes. Using {FogPrime}, the satisfaction of the end-users and the fog nodes increases by 24.49-47.82\%, respectively. Additionally, we observe that {FogPrime} ensures an even distribution of profit among the fog nodes and enables the end-users to pay less at most by 15.88-47.27\%.},
	pages = {8227--8236},
	number = {8},
	journaltitle = {{IEEE} Transactions on Vehicular Technology},
	author = {Misra, Subhas Chandra and Mondal, Ayan},
	date = {2021-08},
	note = {Conference Name: {IEEE} Transactions on Vehicular Technology},
	keywords = {Cloud computing, Resource management, game theory, Games, Delays, Fog computing, Dynamic scheduling, Pricing, dynamic coalition formation, offloading, pricing, utility game, Vehicle dynamics},
	file = {FogPrime\: Dynamic Pricing-Based Strategic Resource Management in Fog Networks:/home/volodia/Zotero/storage/U3JMG95R/misra2021.pdf.pdf:application/pdf;IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/6QEE9G8F/9479709.html:text/html},
}

@inproceedings{junior_stateful_2020,
	title = {Stateful Container Migration in Geo-Distributed Environments},
	url = {https://hal.inria.fr/hal-02963913},
	abstract = {Container migration is an essential functionality in large-scale geo-distributed platforms such as fog computing infrastructures. Contrary to migration within a single data center, long-distance migration requires that the container's disk state should be migrated together with the container itself. However, this state may be arbitrarily large, so its transfer may create long periods of unavailability for the container. We propose to exploit the layered structure provided by the {OverlayFS} file system to transparently snapshot the volumes' contents and transfer them prior to the actual container migration. We implemented this mechanism within Kubernetes. Our evaluations based on a real fog computing test-bed show that our techniques reduce the container's downtime during migration by a factor 4 compared to a baseline with no volume checkpoint.},
	eventtitle = {{CloudCom} 2020 - 12th {IEEE} International Conference on Cloud Computing Technology and Science},
	pages = {1},
	publisher = {{IEEE}},
	author = {Junior, Paulo Souza and Miorandi, Daniele and Pierre, Guillaume},
	urldate = {2021-12-31},
	date = {2020-12-14},
	langid = {english},
	file = {Snapshot:/home/volodia/Zotero/storage/YZ3YST8A/hal-02963913.html:text/html;Full Text PDF:/home/volodia/Zotero/storage/LY5BI9SB/Junior et al. - 2020 - Stateful Container Migration in Geo-Distributed En.pdf:application/pdf},
}

@inproceedings{tamiru_mck8s_2021,
	title = {mck8s: An orchestration platform for geo-distributed multi-cluster environments},
	url = {https://hal.inria.fr/hal-03205743},
	shorttitle = {mck8s},
	abstract = {Following the adoption of cloud computing, the proliferation of cloud data centers in multiple regions, and the emergence of computing paradigms such as fog computing, there is a need for integrated and efficient management of geodistributed clusters. Geo-distributed deployments suffer from resource fragmentation, as the resources in certain locations are over-allocated while others are under-utilized. Orchestration platforms such as Kubernetes and Kubernetes Federation offer the conceptual models and building blocks that can be used to build integrated solutions that address the resource fragmentation challenge. In this work, we propose mck8s-an orchestration platform for multi-cluster applications on multiple geo-distributed Kubernetes clusters. It offers controllers that automatically place, scale, and burst multi-cluster applications across multiple geo-distributed Kubernetes clusters. mck8s allocates the requested resources to all incoming applications while making efficient use of resources. We designed mck8s to be easy to use by development and operation teams by adopting Kubernetes' design principles and manifest files. We evaluated mck8s in a geo-distributed experimental testbed in Grid'5000. Our results show that mck8s balances the resource allocation across multiple clusters and reduces the fraction of pending pods to 6\% as opposed to 65\% in the case of Kubernetes Federation for the same workload.},
	eventtitle = {{ICCCN} 2021 - 30th International Conference on Computer Communications and Networks},
	pages = {1},
	author = {Tamiru, Mulugeta and Pierre, Guillaume and Tordsson, Johan and Elmroth, Erik},
	urldate = {2021-12-31},
	date = {2021-07-19},
	langid = {english},
	file = {Snapshot:/home/volodia/Zotero/storage/RYKUC5VH/hal-03205743.html:text/html;Full Text PDF:/home/volodia/Zotero/storage/Z42WYC5G/Tamiru et al. - 2021 - mck8s An orchestration platform for geo-distribut.pdf:application/pdf},
}

@article{arkian_potable_2020,
	title = {Potable Water Management with integrated Fog computing and {LoRaWAN} technologies},
	url = {https://hal.inria.fr/hal-02513467},
	pages = {1},
	journaltitle = {{IEEE} {IoT} Newsletter},
	author = {Arkian, Hamidreza and Giouroukis, Dimitrios and Junior, Paulo Souza and Pierre, Guillaume},
	urldate = {2021-12-31},
	date = {2020-03-11},
	langid = {english},
	file = {Snapshot:/home/volodia/Zotero/storage/CXCS7B26/hal-02513467.html:text/html;Full Text PDF:/home/volodia/Zotero/storage/6VET8PBG/Arkian et al. - 2020 - Potable Water Management with integrated Fog compu.pdf:application/pdf},
}

@inproceedings{tavonatti_experimental_2021,
	title = {An experimental evaluation of the scalability of permissioned blockchains},
	url = {https://hal.inria.fr/hal-03263551},
	abstract = {Permissioned blockchains are decentralized digital systems, which are used to record transactions and which maintain multiple, synchronized copies of the whole list of transactions (i.e., the ledger) on geographically dispersed nodes. Cryptographic operations are used to 'chain' transactions in the ledger, making the system tamper-resistant. In permissioned blockchains, access to the system (in particular in terms of the ability to append new transactions to the ledger) is limited to a specific set of well-identified nodes: this feature puts them apart from the blockchain systems ('permissionless') commonly use to power cryptocurrencies. The ability to control who can operate on the blockchain makes such systems a good choice for implementing use cases like supply chain management, business ecosystems or notarization. Driven by the interest in launching a new digital product for the education market (related to the management of education certificates), we faced some issues related to the scalability of permissioned blockchains. Given the lack of a consistent and comprehensive literature on the subject, we run an extensive experimental testing campaign on a large-scale distributed computing infrastructure (Grid'5000), measuring the performance of a popular permissioned blockchain framework (Hyperledger Fabric) under varying conditions. In this paper, we share the results obtained, which shed light on both scalability bottlenecks and possible approaches for overcoming such limitations in real-world business contexts.},
	eventtitle = {{FiCloud} 2021 - 8th International Conference on Future Internet of Things and Cloud},
	pages = {1},
	author = {Tavonatti, Stefano and Battulga, Davaadorj and Farhadi, Mozhdeh and Caprini, Carlo and Miorandi, Daniele},
	urldate = {2021-12-31},
	date = {2021-08-23},
	langid = {english},
	file = {Snapshot:/home/volodia/Zotero/storage/Z8ZUCSQV/hal-03263551.html:text/html;Full Text PDF:/home/volodia/Zotero/storage/P9GDSTTQ/Tavonatti et al. - 2021 - An experimental evaluation of the scalability of p.pdf:application/pdf},
}

@article{ahmed_fog_2019,
	title = {Fog Computing Applications: Taxonomy and Requirements},
	url = {http://arxiv.org/abs/1907.11621},
	shorttitle = {Fog Computing Applications},
	abstract = {Fog computing was designed to support the specific needs of latency-critical applications such as augmented reality, and {IoT} applications which produce massive volumes of data that are impractical to send to faraway cloud data centers for analysis. However this also created new opportunities for a wider range of applications which in turn impose their own requirements on future fog computing platforms. This article presents a study of a representative set of 30 fog computing applications and the requirements that a general-purpose fog computing platform should support.},
	journaltitle = {{arXiv}:1907.11621 [cs]},
	author = {Ahmed, Arif and Arkian, {HamidReza} and Battulga, Davaadorj and Fahs, Ali J. and Farhadi, Mozhdeh and Giouroukis, Dimitrios and Gougeon, Adrien and Gutierrez, Felipe Oliveira and Pierre, Guillaume and Souza Jr, Paulo R. and Tamiru, Mulugeta Ayalew and Wu, Li},
	urldate = {2021-12-31},
	date = {2019-07-26},
	eprinttype = {arxiv},
	eprint = {1907.11621},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv.org Snapshot:/home/volodia/Zotero/storage/72KEHS7D/1907.html:text/html;arXiv Fulltext PDF:/home/volodia/Zotero/storage/HGRTSTRM/Ahmed et al. - 2019 - Fog Computing Applications Taxonomy and Requireme.pdf:application/pdf},
}

@online{noauthor_measurement-driven_nodate,
	title = {‪Measurement-driven design and runtime optimization in edge computing: Methodology and tools‬},
	url = {https://scholar.google.com/citations?view_op=view_citation&hl=fr&user=sTVmHWUAAAAJ&sortby=pubdate&alert_preview_top_rm=2&citation_for_view=sTVmHWUAAAAJ:9NZAP19TdFAC},
	shorttitle = {‪Measurement-driven design and runtime optimization in edge computing},
	abstract = {‪C Caiazza, C Cicconetti, V Luconi, A Vecchio‬, ‪Computer Networks, 2021‬ - ‪Cité(e) 1 fois‬},
	urldate = {2021-12-30},
	file = {Snapshot:/home/volodia/Zotero/storage/V5DNIRQU/citations.html:text/html},
}

@article{puliafito_stateful_2021-1,
	title = {Stateful Function-as-a-Service at the Edge},
	url = {http://arxiv.org/abs/2109.15040},
	abstract = {In {FaaS}, an application is decomposed into functions. When functions are stateful, they typically need to remotely access the state, via an external service. On the one hand, this approach makes function instances equivalent to one another, which provides great resource efﬁciency. On the other hand, accessing a remote state causes increased delays and network trafﬁc, which makes {FaaS} less attractive to edge computing systems. We propose to generalize {FaaS} by allowing functions to alternate between remote-state and local-state phases, depending on internal and external conditions, and dedicating a container with persistent memory to functions when in a local-state phase. We present initial results showing that this simple yet powerful pattern allows to better utilize the available resources, which are scarce on edge nodes, while signiﬁcantly reducing tail latencies, which is key to enable new applications based on real-time Machine Learning ({ML}), e.g., in smart vehicles and smart factory scenarios.},
	journaltitle = {{arXiv}:2109.15040 [cs]},
	author = {Puliafito, Carlo and Cicconetti, Claudio and Conti, Marco and Mingozzi, Enzo and Passarella, Andrea},
	urldate = {2021-12-30},
	date = {2021-12-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2109.15040},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Networking and Internet Architecture},
	file = {Puliafito et al. - 2021 - Stateful Function-as-a-Service at the Edge.pdf:/home/volodia/Zotero/storage/43PBJ8RD/Puliafito et al. - 2021 - Stateful Function-as-a-Service at the Edge.pdf:application/pdf},
}

@article{cicconetti_faas_2021,
	title = {{FaaS} Execution Models for Edge Applications},
	url = {http://arxiv.org/abs/2111.06595},
	abstract = {In this paper, we address the problem of supporting stateful workﬂows following a Function-as-a-Service ({FaaS}) model in edge networks. In particular we focus on the problem of data transfer, which can be a performance bottleneck due to the limited speed of communication links in some edge scenarios and we propose three different schemes: a pure {FaaS} implementation, {StateProp}, i.e., propagation of the application state throughout the entire chain of functions, and {StateLocal}, i.e., a solution where the state is kept local to the workers that run functions and retrieved only as needed. We then extend the proposed schemes to the more general case of applications modeled as Directed Acyclic Graphs ({DAGs}), which cover a broad range of practical applications, e.g., in the Internet of Things ({IoT}) area. Our contribution is validated via a prototype implementation. Experiments in emulated conditions show that applying the data locality principle reduces signiﬁcantly the volume of network trafﬁc required and improves the end-to-end delay performance, especially with local caching on edge nodes and low link speeds.},
	journaltitle = {{arXiv}:2111.06595 [cs]},
	author = {Cicconetti, Claudio and Conti, Marco and Passarella, Andrea},
	urldate = {2021-12-30},
	date = {2021-11-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2111.06595},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Networking and Internet Architecture},
	file = {Cicconetti et al. - 2021 - FaaS Execution Models for Edge Applications.pdf:/home/volodia/Zotero/storage/AAVASTKH/Cicconetti et al. - 2021 - FaaS Execution Models for Edge Applications.pdf:application/pdf},
}

@online{noauthor_faas_nodate,
	title = {‪{FaaS} Execution Models for Edge Applications‬},
	url = {https://scholar.google.com/citations?view_op=view_citation&hl=fr&user=sTVmHWUAAAAJ&sortby=pubdate&alert_preview_top_rm=2&citation_for_view=sTVmHWUAAAAJ:TesyEGJKHF4C},
	abstract = {‪C Cicconetti, M Conti, A Passarella‬, ‪{arXiv} preprint {arXiv}:2111.06595, 2021‬},
	urldate = {2021-12-30},
	file = {Snapshot:/home/volodia/Zotero/storage/FUKA58M3/citations.html:text/html},
}

@inreference{wikipedia_hungarian_2021,
	title = {Hungarian algorithm},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Hungarian_algorithm&oldid=1055332388},
	abstract = {The Hungarian method is a combinatorial optimization algorithm that solves the assignment problem in polynomial time and which anticipated later primal–dual methods. It was developed and published in 1955 by Harold Kuhn, who gave the name "Hungarian method" because the algorithm was largely based on the earlier works of two Hungarian mathematicians: Dénes Kőnig and Jenő Egerváry.James Munkres reviewed the algorithm in 1957 and observed that it is (strongly) polynomial. Since then the algorithm has been known also as the Kuhn–Munkres algorithm or Munkres assignment algorithm. The time complexity of the original algorithm was 
  
    
      
        O
        (
        
          n
          
            4
          
        
        )
      
    
    \{{\textbackslash}displaystyle O(n{\textasciicircum}\{4\})\}
  , however Edmonds and Karp, and independently Tomizawa noticed that it can be modified to achieve an 
  
    
      
        O
        (
        
          n
          
            3
          
        
        )
      
    
    \{{\textbackslash}displaystyle O(n{\textasciicircum}\{3\})\}
   running time. One of the most popular 
  
    
      
        O
        (
        
          n
          
            3
          
        
        )
      
    
    \{{\textbackslash}displaystyle O(n{\textasciicircum}\{3\})\}
   variants is the Jonker–Volgenant algorithm. Ford and Fulkerson extended the method to general maximum flow problems in form of the Ford–Fulkerson algorithm. In 2006, it was discovered that Carl Gustav Jacobi had solved the assignment problem in the 19th century, and the solution had been published posthumously in 1890 in Latin.},
	booktitle = {Wikipedia},
	author = {{Wikipedia}},
	urldate = {2021-12-30},
	date = {2021-11-15},
	langid = {english},
	note = {Page Version {ID}: 1055332388},
	file = {Snapshot:/home/volodia/Zotero/storage/IFJGTIZN/index.html:text/html},
}

@article{shi_edge_2016,
	title = {Edge Computing: Vision and Challenges},
	volume = {3},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2016.2579198},
	shorttitle = {Edge Computing},
	abstract = {The proliferation of Internet of Things ({IoT}) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.},
	pages = {637--646},
	number = {5},
	journaltitle = {{IEEE} Internet of Things Journal},
	author = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
	date = {2016-10},
	note = {Conference Name: {IEEE} Internet of Things Journal},
	keywords = {Cloud computing, Edge computing, Bandwidth, Data privacy, Internet of things, Internet of Things ({IoT}), Mobile handsets, smart home and city, Smart homes, Time factors},
	file = {Edge Computing\: Vision and Challenges:/home/volodia/Zotero/storage/42CGZKKN/shi2016.pdf.pdf:application/pdf;IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/MDDBRVUS/7488250.html:text/html},
}

@article{baresi_paps_2021,
	title = {{PAPS}: A Serverless Platform for Edge Computing Infrastructures},
	volume = {3},
	issn = {2624-9634},
	url = {https://www.frontiersin.org/article/10.3389/frsc.2021.690660},
	doi = {10.3389/frsc.2021.690660},
	shorttitle = {{PAPS}},
	abstract = {Edge computing infrastructures are often employed to run applications with low latency requirements. Users can exploits nodes that are close to their physical positions so that the delay of sending computations and data to the Cloud is mitigated. Since users frequently change their locations, and the resources available in the Edge are limited, the management of these infrastructures poses new, difficult challenges. This paper presents {PAPS} (Partitioning, Allocation, Placement, and Scaling), a framework for the efficient, automated and scalable management of large-scale Edge topologies. {PAPS} acts as a serveless platform for the Edge. Service providers can upload applications as compositions of lightweight and stateless functions along with latency constraints. At runtime, {PAPS} manages these applications by executing them in containers, it changes their placement in the Edge topology according to the geographical distribution of the workload, and efficiently allocates resources according to their needs. This paper also presents the architecture of a {PAPS} prototype built atop Kubernetes and {OpenFaaS}. The assessment shows both the feasibility of the approach and the ability of efficiently managing hundreds of serverless concurrent functions and of dealing with intense and unpredictable workload variations.},
	pages = {58},
	journaltitle = {Frontiers in Sustainable Cities},
	author = {Baresi, Luciano and Quattrocchi, Giovanni},
	urldate = {2021-12-28},
	date = {2021},
	file = {Full Text PDF:/home/volodia/Zotero/storage/XLIJASD3/Baresi and Quattrocchi - 2021 - PAPS A Serverless Platform for Edge Computing Inf.pdf:application/pdf},
}

@inproceedings{xu_zenith_2017,
	title = {Zenith: Utility-Aware Resource Allocation for Edge Computing},
	doi = {10.1109/IEEE.EDGE.2017.15},
	shorttitle = {Zenith},
	abstract = {In the Internet of Things({IoT}) era, the demands for low-latency computing for time-sensitive applications (e.g., location-based augmented reality games, real-time smart grid management, real-time navigation using wearables) has been growing rapidly. Edge Computing provides an additional layer of infrastructure to fill latency gaps between the {IoT} devices and the back-end computing infrastructure. In the edge computing model, small-scale micro-datacenters that represent ad-hoc and distributed collection of computing infrastructure pose new challenges in terms of management and effective resource sharing to achieve a globally efficient resource allocation. In this paper, we propose Zenith, a novel model for allocating computing resources in an edge computing platform that allows service providers to establish resource sharing contracts with edge infrastructure providers apriori. Based on the established contracts, service providers employ a latency-aware scheduling and resource provisioning algorithm that enables tasks to complete and meet their latency requirements. The proposed techniques are evaluated through extensive experiments that demonstrate the effectiveness, scalability and performance efficiency of the proposed model.},
	eventtitle = {2017 {IEEE} International Conference on Edge Computing ({EDGE})},
	pages = {47--54},
	booktitle = {2017 {IEEE} International Conference on Edge Computing ({EDGE})},
	author = {Xu, Jinlai and Palanisamy, Balaji and Ludwig, Heiko and Wang, Qingyang},
	date = {2017-06},
	keywords = {Cloud computing, Edge computing, Resource management, Computational modeling, fog computing, resource allocation, edge computing, Containers, Contracts, Logic gates},
	file = {Zenith\: Utility-Aware Resource Allocation for Edge Computing:/home/volodia/Zotero/storage/8F42HK3V/xu2017.pdf.pdf:application/pdf},
}

@article{cao_edge_2020,
	title = {Edge Federation: Towards an Integrated Service Provisioning Model},
	volume = {28},
	issn = {1063-6692, 1558-2566},
	url = {http://arxiv.org/abs/1902.09055},
	doi = {10.1109/TNET.2020.2979361},
	shorttitle = {Edge Federation},
	abstract = {Edge computing is a promising computing paradigm for pushing the cloud service to the network edge. To this end, edge infrastructure providers ({EIPs}) need to bring computation and storage resources to the network edge and allow edge service providers ({ESPs}) to provision latency-critical services to users. Currently, {EIPs} prefer to establish a series of private edge-computing environments to serve specific requirements of users. This kind of resource provisioning mechanism severely limits the development and spread of edge computing for serving diverse user requirements. To this end, we propose an integrated resource provisioning model, named edge federation, to seamlessly realize the resource cooperation and service provisioning across standalone edge computing providers and clouds. To efficiently schedule and utilize the resources across multiple {EIPs}, we systematically characterize the provisioning process as a large-scale linear programming ({LP}) problem and transform it into an easily solved form. Accordingly, we design a dynamic algorithm to tackle the varying service demands from users. We conduct extensive experiments over the base station networks in Toronto city. Compared with the existing fixed contract model and multihoming model, edge federation can reduce the overall cost of {EIPs} by 23.3\% to 24.5\%, and 15.5\% to 16.3\%, respectively.},
	pages = {1116--1129},
	number = {3},
	journaltitle = {{IEEE}/{ACM} Transactions on Networking},
	shortjournal = {{IEEE}/{ACM} Trans. Networking},
	author = {Cao, Xiaofeng and Tang, Guoming and Guo, Deke and Li, Yan and Zhang, Weiming},
	urldate = {2022-01-03},
	date = {2020-06},
	eprinttype = {arxiv},
	eprint = {1902.09055},
	keywords = {Computer Science - Networking and Internet Architecture},
	file = {Edge Federation\: Towards an Integrated Service Provisioning Model:/home/volodia/Zotero/storage/7G5S6KTV/cao2020.pdf.pdf:application/pdf;arXiv Fulltext PDF:/home/volodia/Zotero/storage/A4QXZ3KV/Cao et al. - 2020 - Edge Federation Towards an Integrated Service Pro.pdf:application/pdf},
}

@article{yousefpour_all_2019,
	title = {All one needs to know about fog computing and related edge computing paradigms: A complete survey},
	volume = {98},
	issn = {1383-7621},
	url = {https://www.sciencedirect.com/science/article/pii/S1383762118306349},
	doi = {10.1016/j.sysarc.2019.02.009},
	shorttitle = {All one needs to know about fog computing and related edge computing paradigms},
	abstract = {With the Internet of Things ({IoT}) becoming part of our daily life and our environment, we expect rapid growth in the number of connected devices. {IoT} is expected to connect billions of devices and humans to bring promising advantages for us. With this growth, fog computing, along with its related edge computing paradigms, such as multi-access edge computing ({MEC}) and cloudlet, are seen as promising solutions for handling the large volume of security-critical and time-sensitive data that is being produced by the {IoT}. In this paper, we first provide a tutorial on fog computing and its related computing paradigms, including their similarities and differences. Next, we provide a taxonomy of research topics in fog computing, and through a comprehensive survey, we summarize and categorize the efforts on fog computing and its related computing paradigms. Finally, we provide challenges and future directions for research in fog computing.},
	pages = {289--330},
	journaltitle = {Journal of Systems Architecture},
	shortjournal = {Journal of Systems Architecture},
	author = {Yousefpour, Ashkan and Fung, Caleb and Nguyen, Tam and Kadiyala, Krishna and Jalali, Fatemeh and Niakanlahiji, Amirreza and Kong, Jian and Jue, Jason P.},
	urldate = {2022-01-07},
	date = {2019-09-01},
	langid = {english},
	keywords = {Cloud computing, Edge computing, Fog computing, Cloudlet, Internet of things ({IoT}), Mist computing, Mobile edge computing, Multi-access edge computing},
	file = {Yousefpour et al_2019_All one needs to know about fog computing and related edge computing paradigms.pdf:/home/volodia/Zotero/storage/8X8IXI3B/Yousefpour et al_2019_All one needs to know about fog computing and related edge computing paradigms.pdf:application/pdf},
}

@article{r_sreekanth_mobile_2022,
	title = {Mobile Fog Computing by Using {SDN}/{NFV} on 5G Edge Nodes},
	volume = {41},
	issn = {0267-6192},
	url = {https://www.techscience.com/csse/v41n2/45191},
	doi = {10.32604/csse.2022.020534},
	abstract = {Fog computing provides quality of service for cloud infrastructure. As the data computation intensiﬁes, edge computing becomes difﬁcult. Therefore, mobile fog computing is used for reducing trafﬁc and the time for data computation in the network. In previous studies, software-deﬁned networking ({SDN}) and network functions virtualization ({NFV}) were used separately in edge computing. Current industrial and academic research is tackling to integrate {SDN} and {NFV} in different environments to address the challenges in performance, reliability, and scalability. {SDN}/{NFV} is still in development. The traditional Internet of things ({IoT}) data analysis system is only based on a linear and time-variant system that needs an {IoT} data system with a high-precision model. This paper proposes a combined architecture of {SDN} and {NFV} on an edge node server for {IoT} devices to reduce the computational complexity in cloud-based fog computing. {SDN} provides a generalization structure of the forwarding plane, which is separated from the control plane. Meanwhile, {NFV} concentrates on virtualization by combining the forwarding model with virtual network functions ({VNFs}) as a single or chain of {VNFs}, which leads to interoperability and consistency. The orchestrator layer in the proposed software-deﬁned {NFV} is responsible for handling real-time tasks by using an edge node server through the {SDN} controller via four actions: task creation, modiﬁcation, operation, and completion. Our proposed architecture is simulated on the {EstiNet} simulator, and total time delay, reliability, and satisfaction are used as evaluation parameters. The simulation results are compared with the results of existing architectures, such as software-deﬁned uniﬁed virtual monitoring function and {ASTP}, to analyze the performance of the proposed architecture. The analysis results indicate that our proposed architecture achieves better performance in terms of total time delay (1800 s for 200 {IoT} devices), reliability (90\%), and satisfaction (90\%).},
	pages = {751--765},
	number = {2},
	journaltitle = {Computer Systems Science and Engineering},
	author = {R. Sreekanth, G. and Ahmed Najat Ahmed, S. and Sarac, Marko and Strumberger, Ivana and Bacanin, Nebojsa and Zivkovic, Miodrag},
	urldate = {2022-01-14},
	date = {2022},
	langid = {english},
	file = {R. Sreekanth et al. - 2022 - Mobile Fog Computing by Using SDNNFV on 5G Edge N.pdf:/home/volodia/Zotero/storage/QYNUNCUI/R. Sreekanth et al. - 2022 - Mobile Fog Computing by Using SDNNFV on 5G Edge N.pdf:application/pdf},
}

@inproceedings{bermbach_towards_2021,
	location = {New York, {NY}, {USA}},
	title = {Towards grassroots peering at the edge},
	isbn = {978-1-4503-9167-2},
	url = {https://doi.org/10.1145/3493369.3493602},
	doi = {10.1145/3493369.3493602},
	series = {M4IoT '21},
	abstract = {Fog Computing allows applications to address their latency and privacy requirements while coping with bandwidth limitations of Internet service providers ({ISPs}). Existing research on fog systems has so far mostly taken a very high-level view on the actual fog infrastructure. In this position paper, we identify and discuss the problem of having multiple {ISPs} in edge-to-edge communication. As a possible solution we propose that edge operators create direct edge-to-edge links in a grassroots fashion and discuss different implementation options. Based on this, we highlight some important open research challenges that result from this.},
	pages = {14--17},
	booktitle = {Proceedings of the 8th International Workshop on Middleware and Applications for the Internet of Things},
	publisher = {Association for Computing Machinery},
	author = {Bermbach, David and Lucia, Sergio and Handziski, Vlado and Wolisz, Adam},
	urldate = {2022-01-12},
	date = {2021-12-06},
	keywords = {fog computing, edge computing, network peering},
}

@report{milgrom_redesigning_2017,
	location = {Rochester, {NY}},
	title = {Redesigning Spectrum Licenses to Encourage Innovation and Investment},
	url = {https://papers.ssrn.com/abstract=3015929},
	abstract = {Commercial radio spectrum use rights in the {US} are traditionally assigned using licenses over large geographic areas with 10- or 15-year terms, to encourage infrastructure investment. However, such long-term licenses are difficult to reassign as more valuable uses for spectrum arise. Licenses with shorter term limits over smaller areas expedite reassignment of spectrum to innovative entrants, but provide lower incentives for long-term investment. Recent economic theory suggests that this trade-off between protecting long-term investments and enabling valuable, innovative entry can be muted by a new, more efficient “depreciating” license. A promising application is to priority access in the 3.5GHz band, where thousands of licenses are about to be auctioned. Alternatively, carefully redesigning auction rules may offer similar benefits.},
	number = {{ID} 3015929},
	institution = {Social Science Research Network},
	type = {{SSRN} Scholarly Paper},
	author = {Milgrom, Paul R. and Weyl, E. Glen and Zhang, Anthony Lee},
	urldate = {2022-01-12},
	date = {2017-08-07},
	langid = {english},
	keywords = {3.5GHz, depreciating license, priority access licenses},
	file = {Milgrom et al_2017_Redesigning Spectrum Licenses to Encourage Innovation and Investment.pdf:/home/volodia/Zotero/storage/IIW5XEHX/Milgrom et al_2017_Redesigning Spectrum Licenses to Encourage Innovation and Investment.pdf:application/pdf;Snapshot:/home/volodia/Zotero/storage/K766UPE5/papers.html:text/html},
}

@online{rausch_cognitivexr_2020,
	title = {{CognitiveXR}},
	url = {https://cognitivexr.at/},
	author = {Rausch, Thomas and Krösl, Katharina},
	urldate = {2022-01-11},
	date = {2020},
	langid = {english},
}

@online{noauthor_cognitive_nodate,
	title = {Cognitive {XR}},
	url = {https://cognitivexr.at/},
	urldate = {2022-01-11},
	file = {Cognitive XR:/home/volodia/Zotero/storage/QQ24TVDC/cognitivexr.at.html:text/html},
}

@inproceedings{rausch_towards_2021,
	location = {Lisbon, Portugal},
	title = {Towards a Platform for Smart City-Scale Cognitive Assistance Applications},
	isbn = {978-1-66544-057-8},
	url = {https://ieeexplore.ieee.org/document/9419192/},
	doi = {10.1109/VRW52623.2021.00066},
	abstract = {This position paper describes {CognitiveAR}, a system that seamlessly interfaces {AR} devices with smart city environments. Edge computing nodes distributed throughout the city enable multi-user cognitive assistance applications that require (1) real-time sensor data from the environment, such as approaching cars, and (2) computing resources for low-latency video processing. We discuss three such applications to elicit requirements for a platform to support them, and present our system design.},
	eventtitle = {2021 {IEEE} Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops ({VRW})},
	pages = {330--335},
	booktitle = {2021 {IEEE} Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops ({VRW})},
	publisher = {{IEEE}},
	author = {Rausch, Thomas and Hummer, Waldemar and Stippel, Christian and Vasiljevic, Silvio and Elvezio, Carmine and Dustdar, Schahram and Krosl, Katharina},
	urldate = {2022-01-11},
	date = {2021-03},
	langid = {english},
	file = {Rausch et al_2021_Towards a Platform for Smart City-Scale Cognitive Assistance Applications.pdf:/home/volodia/Zotero/storage/F83GRYRK/Rausch et al_2021_Towards a Platform for Smart City-Scale Cognitive Assistance Applications.pdf:application/pdf;Rausch et al. - 2021 - Towards a Platform for Smart City-Scale Cognitive .pdf:/home/volodia/Zotero/storage/ND8HF5HK/Rausch et al. - 2021 - Towards a Platform for Smart City-Scale Cognitive .pdf:application/pdf},
}

@inproceedings{deyannis_enabling_2018,
	location = {New York, {NY}, {USA}},
	title = {Enabling {GPU}-assisted Antivirus Protection on Android Devices through Edge Offloading},
	isbn = {978-1-4503-5837-8},
	url = {https://doi.org/10.1145/3213344.3213347},
	doi = {10.1145/3213344.3213347},
	series = {{EdgeSys}'18},
	abstract = {Antivirus software are the most popular tools for detecting and stopping malicious or unwanted files. However, the performance requirements of traditional host-based antivirus make their wide adoption to mobile, embedded, and hand-held devices questionable. Their computational- and memory-intensive characteristics, which are needed to cope with the evolved and sophisticated malware, makes their deployment to mobile processors a hard task. Moreover, their increasing complexity may result in vulnerabilities that can be exploited by malware. In this paper, we first describe a {GPU}-based antivirus algorithm for Android devices. Then, due to the limited number of {GPU}-enabled Android devices, we present different architecture designs that exploit code offloading for running the antivirus on more powerful machines. This approach enables lower execution and memory overheads, better performance, and improved deployability and management. We evaluate the performance, scalability, and efficacy of the system in several different scenarios and setups. We show that the time to detect a malware is 8.4 times lower than the typical local execution approach.},
	pages = {13--18},
	booktitle = {Proceedings of the 1st International Workshop on Edge Systems, Analytics and Networking},
	publisher = {Association for Computing Machinery},
	author = {Deyannis, Dimitris and Tsirbas, Rafail and Vasiliadis, Giorgos and Montella, Raffaele and Kosta, Sokol and Ioannidis, Sotiris},
	urldate = {2022-01-11},
	date = {2018-06-10},
	keywords = {Edge Computing, Android, {CUDA}, {GPGPU}, Malware Detection, Mobile, Offloading},
	file = {Deyannis et al_2018_Enabling GPU-assisted Antivirus Protection on Android Devices through Edge.pdf:/home/volodia/Zotero/storage/T9GLEE6T/Deyannis et al_2018_Enabling GPU-assisted Antivirus Protection on Android Devices through Edge.pdf:application/pdf},
}

@online{noauthor_swamp_nodate,
	title = {{SWAMP} – Smart Water Management Platform},
	url = {https://swamp-project.org/},
	urldate = {2022-01-11},
	langid = {english},
	file = {Snapshot:/home/volodia/Zotero/storage/NZB28N92/swamp-project.org.html:text/html},
}

@inproceedings{chen_design_2018,
	title = {Design and implementation of a power consumption management system for smart home over fog-cloud computing},
	doi = {10.1109/IGBSG.2018.8393553},
	abstract = {Smart home becomes a part of the trend in a vast Internet of Things ({IoT}) field. However, according to International Data Corporation, {IDC}, every house generated 2 {TB} amount of data on average by 2014, and this number is estimated to rise up to 10 {TB} by 2020. With a huge amount of data, cloud-only architecture could not keep up with the volume and velocity of this data across the network. Therefore, we provide the reference architecture for smart home which is based on fog computing using Zigbee protocol. We can treat fog computing as the extension of the cloud. It can ease a load of cloud, improve its performance and efficiency, and also provide real-time calculating service. Furthermore, the cloud can provide insight and configuration to fog computing architecture to enhance the home-automated function and optimize system intelligent. Our approach can effectively address the data amount issues and reduce the response latency to secure the safety for the family member in real-time processing needed situation, and monitor the usage of electricity for each member to implement the home energy management system. Each home fog network can communicate with each other and become a bigger architecture for the smart city in the future.},
	eventtitle = {2018 3rd International Conference on Intelligent Green Building and Smart Grid ({IGBSG})},
	pages = {1--5},
	booktitle = {2018 3rd International Conference on Intelligent Green Building and Smart Grid ({IGBSG})},
	author = {Chen, Yan-Da and Azhari, Muhammad Zulfan and Leu, Jenq-Shiou},
	date = {2018-04},
	keywords = {Cloud computing, Computer architecture, Edge computing, {IoT}, Peer-to-peer computing, Protocols, Fog computing, Smart homes, Smart city, Smart home, Zigbee, {ZigBee}},
	file = {Chen et al_2018_Design and implementation of a power consumption management system for smart.pdf:/home/volodia/Zotero/storage/GEDFXLSE/Chen et al_2018_Design and implementation of a power consumption management system for smart.pdf:application/pdf;IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/ELM5PCQ4/8393553.html:text/html;Chen et al_2018_Design and implementation of a power consumption management system for smart.pdf:/home/volodia/Zotero/storage/ZQVPQFBY/Chen et al_2018_Design and implementation of a power consumption management system for smart.pdf:application/pdf},
}

@misc{openfog_consortium_real-time_2018,
	title = {Real-time subsurface imaging},
	url = {http://www.fogguru.eu/tmp/OpenFog-Use-Cases.zip},
	author = {{OpenFog Consortium}},
	date = {2018},
}

@misc{openfog_consortium_process_2018,
	title = {Process manufacturing for beverage industry},
	url = {http://www.fogguru.eu/tmp/OpenFog-Use-Cases.zip},
	author = {{OpenFog Consortium}},
	date = {2018},
}

@misc{openfog_consortium_out_2018,
	title = {Out of the fog: Use case scenar- ios (high-scale drone package delivery)},
	url = {http://www.fogguru.eu/tmp/OpenFog-Use-Cases.zip},
	author = {{OpenFog Consortium}},
	date = {2018},
}

@article{ieee_standards_association_smart_2018,
	title = {Smart Cities scenario (3.3)},
	doi = {10.1109/IEEESTD.2018.8423800},
	abstract = {{OpenFog} Consortium–{OpenFog} Reference Architecture for Fog Computing is adopted by this standard. {OpenFog} Reference Architecture [{OPFRA}001.020817] is a structural and functional prescription of an open, interoperable, horizontal system architecture for distributing computing, storage, control and networking functions closer to the users along a cloud-to-thing continuum of communicating, computing, sensing and actuating entities. It encompasses various approaches to disperse Information Technology ({IT}), Communication Technology ({CT}) and Operational Technology ({OT}) Services through information messaging infrastructure as well as legacy and emerging multi-access networking technologies.},
	pages = {1--176},
	journaltitle = {{IEEE} Std 1934-2018},
	author = {{IEEE Standards Association}},
	date = {2018-08},
	note = {Conference Name: {IEEE} Std 1934-2018},
	keywords = {Edge computing, adoption, communication technology {IEEE} 1934™, Communications technology, {IEEE} Standards, information technology, Information technology, {OpenFog}, operational technology},
	file = {IEEE Standards Association_2018_Smart Cities scenario (3.pdf:/home/volodia/Zotero/storage/KNJCSD4U/IEEE Standards Association_2018_Smart Cities scenario (3.pdf:application/pdf},
}

@article{lin_cloudfog_2017,
	title = {{CloudFog}: Leveraging Fog to Extend Cloud Gaming for Thin-Client {MMOG} with High Quality of Service},
	volume = {28},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2016.2563428},
	shorttitle = {{CloudFog}},
	abstract = {With the increasing popularity of Massively Multiplayer Online Game ({MMOG}) and fast growth of mobile gaming, cloud gaming exhibits great promises over the conventional {MMOG} gaming model as it frees players from the requirement of hardware and game installation on their local computers. However, as the graphics rendering is offloaded to the cloud, the data transmission between the end-users and the cloud significantly increases the response latency and limits the user coverage, thus preventing cloud gaming to achieve high user Quality of Service ({QoS}). To solve this problem, previous research suggested deploying more datacenters, but it comes at a prohibitive cost. We propose a lightweight system called {CloudFog}, which incorporates “fog” consisting of supernodes that are responsible for rendering game videos and streaming them to their nearby players. Fog enables the cloud to be only responsible for the intensive game state computation and sending update information to supernodes, which significantly reduce the traffic hence the latency and bandwidth consumption. To further enhance {QoS}, we propose the reputation based supernode selection strategy to assign each player with a suitable supernode that can provide satisfactory game video streaming service, the receiver-driven encoding rate adaptation strategy to increase the playback continuity, the social network based server assignment strategy to avoid the communication interaction between servers in a datacenter to reduce latency, and the dynamic supernode provisioning strategy to deal with user churns. Experimental results from {PeerSim} and {PlanetLab} show the effectiveness and efficiency of {CloudFog} and our individual strategies in increasing user coverage, reducing response latency and bandwidth consumption.},
	pages = {431--445},
	number = {2},
	journaltitle = {{IEEE} Transactions on Parallel and Distributed Systems},
	author = {Lin, Yuhua and Shen, Haiying},
	date = {2017-02},
	note = {Conference Name: {IEEE} Transactions on Parallel and Distributed Systems},
	keywords = {Servers, Quality of service, Bandwidth, Cloud gaming, online gaming, P2P network, quality of service, Videos},
	file = {Lin_Shen_2017_CloudFog.pdf:/home/volodia/Zotero/storage/ST4K4GBF/Lin_Shen_2017_CloudFog.pdf:application/pdf;IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/XDKFJUEL/7465785.html:text/html;Lin_Shen_2017_CloudFog.pdf:/home/volodia/Zotero/storage/DCH87J3N/Lin_Shen_2017_CloudFog.pdf:application/pdf},
}

@article{ma_understanding_2017,
	title = {Understanding Performance of Edge Content Caching for Mobile Video Streaming},
	volume = {35},
	issn = {1558-0008},
	doi = {10.1109/JSAC.2017.2680958},
	abstract = {Today's Internet has witnessed an increase in the popularity of mobile video streaming, which is expected to exceed 3/4 of the global mobile data traffic by 2019. To satisfy the considerable amount of mobile video requests, video service providers have been pushing their content delivery infrastructure to edge networks-from regional content delivery network ({CDN}) servers to peer {CDN} servers (e.g., smartrouters in users' homes)-to cache content and serve users with storage and network resources nearby. Among the edge network content caching paradigms, Wi-Fi access point caching and cellular base station caching have become two mainstream solutions. Thus, understanding the effectiveness and performance of these solutions for large-scale mobile video delivery is important. However, the characteristics and request patterns of mobile video streaming are unclear in practical wireless network. In this paper, we use real-world data sets containing 50 million trace items of nearly 2 million users viewing more than 0.3 million unique videos using mobile devices in a metropolis in China over two weeks, not only to understand the request patterns and user behaviors in mobile video streaming, but also to evaluate the effectiveness of Wi-Fi and cellular-based edge content caching solutions. To understand the performance of edge content caching for mobile video streaming, we first present temporal and spatial video request patterns, and we analyze their impacts on caching performance using frequency-domain and entropy analysis approaches. We then study the behaviors of mobile video users, including their mobility and geographical migration behaviors, which determine the request patterns. Using trace-driven experiments, we compare strategies for edge content caching, including least recently used ({LRU}) and least frequently used ({LFU}), in terms of supporting mobile video requests. We reveal that content, location, and mobility factors all affect edge content caching performance. Moreover, we design an efficient caching strategy based on the measurement insights and experimentally evaluate its performance. The results show that our design significantly improves the cache hit rate by up to 30\% compared with {LRU}/{LFU}.},
	pages = {1076--1089},
	number = {5},
	journaltitle = {{IEEE} Journal on Selected Areas in Communications},
	author = {Ma, Ge and Wang, Zhi and Zhang, Miao and Ye, Jiahui and Chen, Minghua and Zhu, Wenwu},
	date = {2017-05},
	note = {Conference Name: {IEEE} Journal on Selected Areas in Communications},
	keywords = {Edge network, Streaming media, Mobile handsets, Base stations, content delivery, measurement, Mobile communication, Mobile computing, mobile video streaming, user behavior, Wireless fidelity, Wireless networks},
	file = {Ma et al_2017_Understanding Performance of Edge Content Caching for Mobile Video Streaming.pdf:/home/volodia/Zotero/storage/RUEFQSV7/Ma et al_2017_Understanding Performance of Edge Content Caching for Mobile Video Streaming.pdf:application/pdf;IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/BZJ5BV2F/7875170.html:text/html;Ma et al_2017_Understanding Performance of Edge Content Caching for Mobile Video Streaming.pdf:/home/volodia/Zotero/storage/GUVR7R29/Ma et al_2017_Understanding Performance of Edge Content Caching for Mobile Video Streaming.pdf:application/pdf},
}

@article{hu_survey_2017,
	title = {Survey on fog computing: architecture, key technologies, applications and open issues},
	volume = {98},
	issn = {1084-8045},
	url = {https://www.sciencedirect.com/science/article/pii/S1084804517302953},
	doi = {10.1016/j.jnca.2017.09.002},
	shorttitle = {Survey on fog computing},
	abstract = {The emergence of Internet of Things ({IoT}) has enabled the interconnection and intercommunication among massive ubiquitous things, which caused an unprecedented generation of huge and heterogeneous amount of data, known as data explosions. On the other hand, although that cloud computing has served as an efficient way to process and store these data, however, challenges, such as the increasing demands of real time or latency-sensitive applications and the limitation of network bandwidth, still cannot be solved by using only cloud computing. Therefore, a new computing paradigm, known as fog computing, has been proposed as a complement to the cloud solution. Fog computing extends the cloud services to the edge of network, and makes computation, communication and storage closer to edge devices and end-users, which aims to enhance low-latency, mobility, network bandwidth, security and privacy. In this paper, we will overview and summarize fog computing model architecture, key technologies, applications, challenges and open issues. Firstly, we will present the hierarchical architecture of fog computing and its characteristics, and compare it with cloud computing and edge computing to emphasize the similarities and differences. Then, the key technologies like computing, communication and storage technologies, naming, resource management, security and privacy protection are introduced to present how to support its deployment and application in a detailed manner. Several application cases like health care, augmented reality, brain machine interface and gaming, smart environments and vehicular fog computing are also presented to further explain fog computing application scenarios. Finally, based on the observation, we propose some challenges and open issues which are worth further in-depth study and research in fog computing development.},
	pages = {27--42},
	journaltitle = {Journal of Network and Computer Applications},
	shortjournal = {Journal of Network and Computer Applications},
	author = {Hu, Pengfei and Dhelim, Sahraoui and Ning, Huansheng and Qiu, Tie},
	urldate = {2022-01-11},
	date = {2017-11-15},
	langid = {english},
	keywords = {Cloud computing, Edge computing, Fog computing, Application, Architecture, Internet of Things({IoT})},
	file = {Hu et al_2017_Survey on fog computing.pdf:/home/volodia/Zotero/storage/2IML4YUL/Hu et al_2017_Survey on fog computing.pdf:application/pdf;ScienceDirect Snapshot:/home/volodia/Zotero/storage/NWX94CRR/S1084804517302953.html:text/html},
}

@inproceedings{hu_live_2017,
	location = {New York, {NY}, {USA}},
	title = {Live Synthesis of Vehicle-Sourced Data Over 4G {LTE}},
	isbn = {978-1-4503-5162-1},
	url = {https://doi.org/10.1145/3127540.3127543},
	doi = {10.1145/3127540.3127543},
	series = {{MSWiM} '17},
	abstract = {Accurate, up-to-date maps of transient traffic and hazards are invaluable to drivers, city managers, and the emerging class of self-driving vehicles. We present {LiveMap}, a scalable, automated system for acquiring, curating, and disseminating detailed, continually-updated road conditions in a region. {LiveMap} leverages in-vehicle cameras, sensors, and processors to crowd-source hazard detection without human intervention. We build a real-time simulation framework that allows a mix of real and simulated components to be tested together at scale. We demonstrate that {LiveMap} can work well at city scales within the limits of today's cellular network bandwidth. We also show the feasibility of accurate, in-vehicle, computer-vision-based hazard detection.},
	pages = {161--170},
	booktitle = {Proceedings of the 20th {ACM} International Conference on Modelling, Analysis and Simulation of Wireless and Mobile Systems},
	publisher = {Association for Computing Machinery},
	author = {Hu, Wenlu and Feng, Ziqiang and Chen, Zhuo and Harkes, Jan and Pillai, Padmanabhan and Satyanarayanan, Mahadev},
	urldate = {2022-01-11},
	date = {2017-11-21},
	keywords = {cloud computing, edge computing, automotive systems, cloudlet, driverless cars, maps, situational awareness, vehicular systems},
	file = {Hu et al_2017_Live Synthesis of Vehicle-Sourced Data Over 4G LTE.pdf:/home/volodia/Zotero/storage/N2MDQJKV/Hu et al_2017_Live Synthesis of Vehicle-Sourced Data Over 4G LTE.pdf:application/pdf;Hu et al_2017_Live Synthesis of Vehicle-Sourced Data Over 4G LTE.pdf:/home/volodia/Zotero/storage/SRQA5HBD/Hu et al_2017_Live Synthesis of Vehicle-Sourced Data Over 4G LTE.pdf:application/pdf},
}

@article{ai_edge_2018,
	title = {Edge computing technologies for Internet of Things: a primer},
	volume = {4},
	issn = {2352-8648},
	url = {https://www.sciencedirect.com/science/article/pii/S2352864817301335},
	doi = {10.1016/j.dcan.2017.07.001},
	shorttitle = {Edge computing technologies for Internet of Things},
	abstract = {With the rapid development of mobile internet and Internet of Things applications, the conventional centralized cloud computing is encountering severe challenges, such as high latency, low Spectral Efficiency ({SE}), and non-adaptive machine type of communication. Motivated to solve these challenges, a new technology is driving a trend that shifts the function of centralized cloud computing to edge devices of networks. Several edge computing technologies originating from different backgrounds to decrease latency, improve {SE}, and support the massive machine type of communication have been emerging. This paper comprehensively presents a tutorial on three typical edge computing technologies, namely mobile edge computing, cloudlets, and fog computing. In particular, the standardization efforts, principles, architectures, and applications of these three technologies are summarized and compared. From the viewpoint of radio access network, the differences between mobile edge computing and fog computing are highlighted, and the characteristics of fog computing-based radio access network are discussed. Finally, open issues and future research directions are identified as well.},
	pages = {77--86},
	number = {2},
	journaltitle = {Digital Communications and Networks},
	shortjournal = {Digital Communications and Networks},
	author = {Ai, Yuan and Peng, Mugen and Zhang, Kecheng},
	urldate = {2022-01-10},
	date = {2018-04-01},
	langid = {english},
	keywords = {Fog computing, Internet of Things ({IoT}), Mobile edge computing, Cloudlets},
	file = {Ai et al_2018_Edge computing technologies for Internet of Things.pdf:/home/volodia/Zotero/storage/TJHE65CD/Ai et al_2018_Edge computing technologies for Internet of Things.pdf:application/pdf;Full Text PDF:/home/volodia/Zotero/storage/KBV968TK/Edge computing technologies for Internet of Things.pdf:application/pdf;ScienceDirect Snapshot:/home/volodia/Zotero/storage/A6YDB3IL/S2352864817301335.html:text/html;Ai et al_2018_Edge computing technologies for Internet of Things.pdf:/home/volodia/Zotero/storage/YL2J84QY/Ai et al_2018_Edge computing technologies for Internet of Things.pdf:application/pdf},
}

@online{redhat_what_2019,
	title = {What is {NFV}?},
	url = {https://www.redhat.com/en/topics/virtualization/what-is-nfv},
	abstract = {Network functions virtualization ({NFV}) is a way to virtualize network services that have traditionally been run on proprietary hardware.},
	author = {{Redhat}},
	urldate = {2022-01-08},
	date = {2019-08},
	langid = {english},
	file = {Snapshot:/home/volodia/Zotero/storage/E7G7YYC5/what-is-nfv.html:text/html},
}

@online{dahmen-lhuissier_etsi_nodate,
	title = {{ETSI} - Our group on Multi-access Edge Computing ({MEC})},
	url = {https://www.etsi.org/committee/1425-mec},
	abstract = {Multi-access Edge Computing ({MEC}) {ISG}},
	titleaddon = {{ETSI}},
	author = {Dahmen-Lhuissier, Sabine},
	urldate = {2022-01-08},
	langid = {english},
	file = {Snapshot:/home/volodia/Zotero/storage/8VRA2M3B/1425-mec.html:text/html},
}

@online{dahmen-lhuissier_etsi_nodate-1,
	title = {{ETSI} - Multi-access Edge Computing - Standards for {MEC}},
	url = {https://www.etsi.org/technologies/multi-access-edge-computing},
	abstract = {The Multi-access Edge Computing ({MEC}) initiative is an Industry Specification Group ({ISG}) within {ETSI}. The purpose of the {ISG} is to create a standardized, open environment which will allow the efficient and seamless integration of applications from vendors, service providers, and third-parties across multi-vendor Multi-access Edge Computing platforms.},
	titleaddon = {{ETSI}},
	author = {Dahmen-Lhuissier, Sabine},
	urldate = {2022-01-08},
	langid = {english},
	file = {Snapshot:/home/volodia/Zotero/storage/P3ILTTWS/multi-access-edge-computing.html:text/html},
}

@article{satyanarayanan_case_2009,
	title = {The Case for {VM}-Based Cloudlets in Mobile Computing},
	volume = {8},
	issn = {1558-2590},
	doi = {10.1109/MPRV.2009.82},
	abstract = {Mobile computing continuously evolve through the sustained effort of many researchers. It seamlessly augments users' cognitive abilities via compute-intensive capabilities such as speech recognition, natural language processing, etc. By thus empowering mobile users, we could transform many areas of human activity. This article discusses the technical obstacles to these transformations and proposes a new architecture for overcoming them. In this architecture, a mobile user exploits virtual machine ({VM}) technology to rapidly instantiate customized service software on a nearby cloudlet and then uses that service over a wireless {LAN}; the mobile device typically functions as a thin client with respect to the service. A cloudlet is a trusted, resource-rich computer or cluster of computers that's well-connected to the Internet and available for use by nearby mobile devices. Our strategy of leveraging transiently customized proximate infrastructure as a mobile device moves with its user through the physical world is called cloudlet-based, resource-rich, mobile computing. Crisp interactive response, which is essential for seamless augmentation of human cognition, is easily achieved in this architecture because of the cloudlet's physical proximity and one-hop network latency. Using a cloudlet also simplifies the challenge of meeting the peak bandwidth demand of multiple users interactively generating and receiving media such as high-definition video and high-resolution images. Rapid customization of infrastructure for diverse applications emerges as a critical requirement, and our results from a proof-of-concept prototype suggest that {VM} technology can indeed help meet this requirement.},
	pages = {14--23},
	number = {4},
	journaltitle = {{IEEE} Pervasive Computing},
	author = {Satyanarayanan, Mahadev and Bahl, Paramvir and Caceres, Ramon and Davies, Nigel},
	date = {2009-10},
	note = {Conference Name: {IEEE} Pervasive Computing},
	keywords = {Cloud computing, Computer architecture, Mobile computing, augmented reality, cognitive augmentation, cyber foraging, dynamic {VM} synthesis, Humans, Internet, Natural language processing, resource constraints, Speech recognition, virtual machines, Virtual machining, Virtual manufacturing, Wireless {LAN}},
	file = {Satyanarayanan et al_2009_The Case for VM-Based Cloudlets in Mobile Computing.pdf:/home/volodia/Zotero/storage/S9JVKIHP/Satyanarayanan et al_2009_The Case for VM-Based Cloudlets in Mobile Computing.pdf:application/pdf;IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/LTJYKAPY/5280678.html:text/html;IEEE Xplore Full Text PDF:/home/volodia/Zotero/storage/W9J5G99V/Satyanarayanan et al. - 2009 - The Case for VM-Based Cloudlets in Mobile Computin.pdf:application/pdf},
}

@inproceedings{bonomi_fog_2012,
	location = {New York, {NY}, {USA}},
	title = {Fog computing and its role in the internet of things},
	isbn = {978-1-4503-1519-7},
	url = {https://doi.org/10.1145/2342509.2342513},
	doi = {10.1145/2342509.2342513},
	series = {{MCC} '12},
	abstract = {Fog Computing extends the Cloud Computing paradigm to the edge of the network, thus enabling a new breed of applications and services. Defining characteristics of the Fog are: a) Low latency and location awareness; b) Wide-spread geographical distribution; c) Mobility; d) Very large number of nodes, e) Predominant role of wireless access, f) Strong presence of streaming and real time applications, g) Heterogeneity. In this paper we argue that the above characteristics make the Fog the appropriate platform for a number of critical Internet of Things ({IoT}) services and applications, namely, Connected Vehicle, Smart Grid, Smart Cities, and, in general, Wireless Sensors and Actuators Networks ({WSANs}).},
	pages = {13--16},
	booktitle = {Proceedings of the first edition of the {MCC} workshop on Mobile cloud computing},
	publisher = {Association for Computing Machinery},
	author = {Bonomi, Flavio and Milito, Rodolfo and Zhu, Jiang and Addepalli, Sateesh},
	urldate = {2022-01-08},
	date = {2012-08-17},
	keywords = {analytics, iot, fog computing, cloud computing, real time systems, software defined networks, wsan},
	file = {Bonomi et al_2012_Fog computing and its role in the internet of things.pdf:/home/volodia/Zotero/storage/KN2TIAB3/Bonomi et al_2012_Fog computing and its role in the internet of things.pdf:application/pdf;Full Text PDF:/home/volodia/Zotero/storage/NB88DGX6/Bonomi et al. - 2012 - Fog computing and its role in the internet of thin.pdf:application/pdf},
}

@article{chiang_fog_2016,
	title = {Fog and {IoT}: An Overview of Research Opportunities},
	volume = {3},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2016.2584538},
	shorttitle = {Fog and {IoT}},
	abstract = {Fog is an emergent architecture for computing, storage, control, and networking that distributes these services closer to end users along the cloud-to-things continuum. It covers both mobile and wireline scenarios, traverses across hardware and software, resides on network edge but also over access networks and among end users, and includes both data plane and control plane. As an architecture, it supports a growing variety of applications, including those in the Internet of Things ({IoT}), fifth-generation (5G) wireless systems, and embedded artificial intelligence ({AI}). This survey paper summarizes the opportunities and challenges of fog, focusing primarily in the networking context of {IoT}.},
	pages = {854--864},
	number = {6},
	journaltitle = {{IEEE} Internet of Things Journal},
	author = {Chiang, Mung and Zhang, Tao},
	date = {2016-12},
	note = {Conference Name: {IEEE} Internet of Things Journal},
	keywords = {fog, Cloud computing, Computer architecture, Edge computing, fog computing, Hardware, Security, Internet of things, Internet of Things ({IoT}), edge networking, edge storage, fog control, fog networking, fog storage},
	file = {Chiang_Zhang_2016_Fog and IoT.pdf:/home/volodia/Zotero/storage/37PA7PUC/Chiang_Zhang_2016_Fog and IoT.pdf:application/pdf;IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/LD7WGAKM/7498684.html:text/html;IEEE Xplore Full Text PDF:/home/volodia/Zotero/storage/KY7BQD4Y/Chiang and Zhang - 2016 - Fog and IoT An Overview of Research Opportunities.pdf:application/pdf},
}

@software{noauthor_api_2022,
	title = {The {API} Traffic Viewer for Kubernetes},
	rights = {Apache-2.0},
	url = {https://github.com/up9inc/mizu},
	abstract = {{API} traffic viewer for Kubernetes enabling you to view all {API} communication between microservices. Think {TCPDump} and Wireshark re-invented for Kubernetes},
	publisher = {{UP}9},
	urldate = {2022-01-08},
	date = {2022-01-08},
	note = {original-date: 2021-04-19T10:29:56Z},
	keywords = {devops, kubernetes, microservice, containers, amqp, cloudnative-services, devops-tools, go, golang, grpc, kafka, microservices, microservices-application, mizu, redis, rest, traffic-viewer, visibility},
}

@article{frost_smart_2017,
	title = {Smart Cities Deserve an Easier Task! Standards Will Help.},
	pages = {20},
	journaltitle = {News from {ETSI}},
	author = {Frost, Lindsay},
	date = {2017},
	langid = {english},
	file = {Frost - Smart Cities Deserve an Easier Task! Standards Wil.pdf:/home/volodia/Zotero/storage/RPUZ96NX/Frost - Smart Cities Deserve an Easier Task! Standards Wil.pdf:application/pdf},
}

@inproceedings{dolui_comparison_2017,
	title = {Comparison of edge computing implementations: Fog computing, cloudlet and mobile edge computing},
	doi = {10.1109/GIOTS.2017.8016213},
	shorttitle = {Comparison of edge computing implementations},
	abstract = {When it comes to storage and computation of large scales of data, Cloud Computing has acted as the de-facto solution over the past decade. However, with the massive growth in intelligent and mobile devices coupled with technologies like Internet of Things ({IoT}), V2X Communications, Augmented Reality ({AR}), the focus has shifted towards gaining real-time responses along with support for context-awareness and mobility. Due to the delays induced on the Wide Area Network ({WAN}) and location agnostic provisioning of resources on the cloud, there is a need to bring the features of the cloud closer to the consumer devices. This led to the birth of the Edge Computing paradigm which aims to provide context aware storage and distributed Computing at the edge of the networks. In this paper, we discuss the three different implementations of Edge Computing namely Fog Computing, Cloudlet and Mobile Edge Computing in detail and compare their features. We define a set of parameters based on which one of these implementations can be chosen optimally given a particular use-case or application and present a decision tree for the selection of the optimal implementation.},
	eventtitle = {2017 Global Internet of Things Summit ({GIoTS})},
	pages = {1--6},
	booktitle = {2017 Global Internet of Things Summit ({GIoTS})},
	author = {Dolui, Koustabh and Datta, Soumya Kanti},
	date = {2017-06},
	keywords = {Cloud computing, Cloud Computing, Computer architecture, Edge computing, Edge Computing, Fog Computing, {IoT}, Real-time systems, Cloudlet, Mobile Edge Computing, Mobile radio mobility management, Performance evaluation},
	file = {Dolui_Datta_2017_Comparison of edge computing implementations.pdf:/home/volodia/Zotero/storage/H63N6KFS/Dolui_Datta_2017_Comparison of edge computing implementations.pdf:application/pdf;IEEE Xplore Abstract Record:/home/volodia/Zotero/storage/8MGKC58Z/8016213.html:text/html;IEEE Xplore Full Text PDF:/home/volodia/Zotero/storage/TL5DTHST/Dolui and Datta - 2017 - Comparison of edge computing implementations Fog .pdf:application/pdf},
}

@article{ren_survey_2019,
	title = {A Survey on End-Edge-Cloud Orchestrated Network Computing Paradigms: Transparent Computing, Mobile Edge Computing, Fog Computing, and Cloudlet},
	volume = {52},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3362031},
	doi = {10.1145/3362031},
	shorttitle = {A Survey on End-Edge-Cloud Orchestrated Network Computing Paradigms},
	abstract = {Sending data to the cloud for analysis was a prominent trend during the past decades, driving cloud computing as a dominant computing paradigm. However, the dramatically increasing number of devices and data traffic in the Internet-of-Things ({IoT}) era are posing significant burdens on the capacity-limited Internet and uncontrollable service delay. It becomes difficult to meet the delay-sensitive and context-aware service requirements of {IoT} applications by using cloud computing alone. Facing these challenges, computing paradigms are shifting from the centralized cloud computing to distributed edge computing. Several new computing paradigms, including Transparent Computing, Mobile Edge Computing, Fog Computing, and Cloudlet, have emerged to leverage the distributed resources at network edge to provide timely and context-aware services. By integrating end devices, edge servers, and cloud, they form a hierarchical {IoT} architecture, i.e., End-Edge-Cloud orchestrated architecture to improve the performance of {IoT} systems. This article presents a comprehensive survey of these emerging computing paradigms from the perspective of end-edge-cloud orchestration. Specifically, we first introduce and compare the architectures and characteristics of different computing paradigms. Then, a comprehensive survey is presented to discuss state-of-the-art research in terms of computation offloading, caching, security, and privacy. Finally, some potential research directions are envisioned for fostering continuous research efforts.},
	pages = {125:1--125:36},
	number = {6},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Ren, Ju and Zhang, Deyu and He, Shiwen and Zhang, Yaoxue and Li, Tao},
	urldate = {2022-01-07},
	date = {2019-10-16},
	keywords = {fog computing, cloudlet, End-edge-cloud orchestration, mobile edge computing, network computing, transparent computing},
	file = {Ren et al_2019_A Survey on End-Edge-Cloud Orchestrated Network Computing Paradigms.pdf:/home/volodia/Zotero/storage/9BE3JEWW/Ren et al_2019_A Survey on End-Edge-Cloud Orchestrated Network Computing Paradigms.pdf:application/pdf},
}

@incollection{baldini_serverless_2017,
	location = {Singapore},
	title = {Serverless Computing: Current Trends and Open Problems},
	isbn = {978-981-10-5026-8},
	url = {https://doi.org/10.1007/978-981-10-5026-8_1},
	shorttitle = {Serverless Computing},
	abstract = {Serverless computing has emerged as a new compelling paradigm for the deployment of applications and services. It represents an evolution of cloud programming models, abstractions, and platforms, and is a testament to the maturity and wide adoption of cloud technologies. In this chapter, we survey existing serverless platforms from industry, academia, and open-source projects, identify key characteristics and use cases, and describe technical challenges and open problems.},
	pages = {1--20},
	booktitle = {Research Advances in Cloud Computing},
	publisher = {Springer},
	author = {Baldini, Ioana and Castro, Paul and Chang, Kerry and Cheng, Perry and Fink, Stephen and Ishakian, Vatche and Mitchell, Nick and Muthusamy, Vinod and Rabbah, Rodric and Slominski, Aleksander and Suter, Philippe},
	editor = {Chaudhary, Sanjay and Somani, Gaurav and Buyya, Rajkumar},
	urldate = {2022-01-18},
	date = {2017},
	langid = {english},
	doi = {10.1007/978-981-10-5026-8_1},
	file = {Baldini et al_2017_Serverless Computing.pdf:/home/volodia/Zotero/storage/6L4RJ5XR/Baldini et al_2017_Serverless Computing.pdf:application/pdf},
}

@article{hellerstein_serverless_2018-1,
	title = {Serverless Computing: One Step Forward, Two Steps Back},
	url = {http://arxiv.org/abs/1812.03651},
	shorttitle = {Serverless Computing},
	abstract = {Serverless computing offers the potential to program the cloud in an autoscaling, pay-as-you go manner. In this paper we address critical gaps in first-generation serverless computing, which place its autoscaling potential at odds with dominant trends in modern computing: notably data-centric and distributed computing, but also open source and custom hardware. Put together, these gaps make current serverless offerings a bad fit for cloud innovation and particularly bad for data systems innovation. In addition to pinpointing some of the main shortfalls of current serverless architectures, we raise a set of challenges we believe must be met to unlock the radical potential that the cloud---with its exabytes of storage and millions of cores---should offer to innovative developers.},
	journaltitle = {{arXiv}:1812.03651 [cs]},
	author = {Hellerstein, Joseph M. and Faleiro, Jose and Gonzalez, Joseph E. and Schleier-Smith, Johann and Sreekanti, Vikram and Tumanov, Alexey and Wu, Chenggang},
	urldate = {2022-01-18},
	date = {2018-12-10},
	eprinttype = {arxiv},
	eprint = {1812.03651},
	keywords = {Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Hellerstein et al_2018_Serverless Computing.pdf:/home/volodia/Zotero/storage/HHZK9UIU/Hellerstein et al_2018_Serverless Computing.pdf:application/pdf},
}

@online{redhat_what_2020,
	title = {What is {FaaS}?},
	url = {https://www.redhat.com/en/topics/cloud-native-apps/what-is-faas},
	abstract = {{FaaS} is a kind of cloud computing service that allows developers to build, run, and manage application packages as functions without maintaining the infrastructure.},
	author = {{RedHat}},
	urldate = {2022-01-18},
	date = {2020-01-03},
	langid = {english},
}

@online{ibm_faas_2019,
	title = {faas},
	url = {https://www.ibm.com/cloud/learn/faas},
	abstract = {An introduction to {FaaS}—a cloud computing service that makes it easier for cloud application developers to run and manage microservices applications.},
	author = {{IBM}},
	urldate = {2022-01-18},
	date = {2019-07-30},
	langid = {english},
}